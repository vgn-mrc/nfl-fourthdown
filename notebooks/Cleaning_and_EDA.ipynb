{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1aafbc5",
   "metadata": {},
   "source": [
    "# NFL - 4th Down EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed/\")\n",
    "\n",
    "# set no limit on rows and columns displayed\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# sns theme\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 8, \"axes.titlesize\": 9, \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7, \"ytick.labelsize\": 7, \"legend.fontsize\": 7,\n",
    "    \"axes.grid\": True, \"grid.linewidth\": 0.4, \"grid.alpha\": 0.3,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 150, \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\", \"savefig.pad_inches\": 0.02\n",
    "})\n",
    "colors = {\"go\":\"#7570b3\", \"fg\":\"#d95f02\", \"punt\":\"#1b9e77\"}\n",
    "\n",
    "WIDTH = 4.0\n",
    "SAVE_FIGS = False\n",
    "SAVE_DIR = \"../reports/figures\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e121af",
   "metadata": {},
   "source": [
    "## 01 Carga de Datos y Volumetría"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c455d8",
   "metadata": {},
   "source": [
    "### Volumetría\n",
    "- Se analizan más de 2,700 juegos de la NFL entre las temporadas de 2014 a 2023\n",
    "- El dataset cuenta con más de 40 mil jugadas de cuarta oportunidad para analizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "nfl_4th = pd.read_parquet(PROCESSED_DATA_PATH / \"4th_down_data.parquet\")\n",
    "nfl_4th = nfl_4th[nfl_4th[\"play_type_nfl\"] != \"PENALTY\"]\n",
    "\n",
    "\n",
    "print(\"Verification of Dataset:\")\n",
    "print(f\" - Columns in the dataset: {nfl_4th.columns.nunique()}\")\n",
    "print(f\" - Verify only 4th down plays are present: {nfl_4th['down'].unique()}\")\n",
    "\n",
    "print(\"\\nVolume:\")\n",
    "print(f\" - Number of unique games: {nfl_4th['game_id'].nunique()}\")\n",
    "print(f\" - Number of 4th down plays: {len(nfl_4th)}\")\n",
    "print(f\" - 4th down plays per game: {len(nfl_4th) / nfl_4th['game_id'].nunique():.2f}\")\n",
    "print(f\" - Seasons covered: {nfl_4th['season'].min()} to {nfl_4th['season'].max()}\")\n",
    "print(f\" - Regular vs Post Season ratio: {nfl_4th['season_type'].value_counts(normalize=True).round(2).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebd654",
   "metadata": {},
   "source": [
    "## 02 Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172644f",
   "metadata": {},
   "source": [
    "### Dominios de variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee357b05",
   "metadata": {},
   "source": [
    "#### - Identificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ead7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "identity_ids = [\n",
    "    \"play_id\", \"game_id\", \"nfl_api_id\", \"stadium_id\", \"nflverse_game_id\",\n",
    "]\n",
    "\n",
    "# Duplicates in primary keys\n",
    "dups = nfl_4th.duplicated(subset=[\"game_id\", \"play_id\"]).sum()\n",
    "print(\"Duplicate (game_id, play_id) pairs:\", dups)\n",
    "print(\"Unique 4th down plays:\", nfl_4th[['game_id', 'play_id']].drop_duplicates().shape[0])\n",
    "print(\"Nulls in primary keys:\", nfl_4th[['game_id', 'play_id']].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5409e4f",
   "metadata": {},
   "source": [
    "#### - Estado del Juego (Game State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = [\n",
    "    \"season\", \"season_type\", \"week\", \"game_date\",\n",
    "    \"qtr\", \"quarter_seconds_remaining\", \"half_seconds_remaining\",\n",
    "    \"game_seconds_remaining\", \"game_half\", \"down\", \"ydstogo\",\n",
    "    \"yardline_100\", \"goal_to_go\", \"play_clock\", \"score_differential\",\n",
    "    \"posteam_timeouts_remaining\", \"defteam_timeouts_remaining\",\n",
    "]\n",
    "\n",
    "print(\"Unique seasons:\", nfl_4th[\"season\"].unique())\n",
    "print(\"Nulls in game state variables:\", nfl_4th[game_state].isna().sum().sum())\n",
    "print(\"Duplicated game_state combinations:\", nfl_4th[game_state].duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c923331",
   "metadata": {},
   "source": [
    "#### - Team Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = [\n",
    "    \"home_team\", \"away_team\", \"posteam\", \"defteam\",\n",
    "    \"posteam_type\", \"side_of_field\", \"home_opening_kickoff\",\n",
    "]\n",
    "\n",
    "print(\"Unique teams:\", nfl_4th[\"home_team\"].nunique())\n",
    "print(\"Nulls in team identity variables:\", nfl_4th[teams].isna().sum().sum())\n",
    "print(\"Plot Team values counts:\")\n",
    "nfl_4th[\"home_team\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529e511",
   "metadata": {},
   "source": [
    "#### - Ambiente de Juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a31e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = [\n",
    "    \"roof\", \"surface\", \"temp\", \"wind\",\n",
    "]\n",
    "\n",
    "print(\"Stadium data overview:\")\n",
    "print(\" - Unique stadiums:\", nfl_4th[\"stadium\"].nunique())\n",
    "print(\" - Type of stadium roofs count:\", nfl_4th[\"roof\"].value_counts().to_dict())\n",
    "print(\" - Type of playing surfaces count:\", nfl_4th[\"surface\"].value_counts().to_dict())\n",
    "\n",
    "roof_map = {\n",
    "    \"closed\": \"indoor\",\n",
    "    \"dome\": \"indoor\",\n",
    "    \"outdoors\": \"open_air\",\n",
    "    \"open\": \"open_air\",\n",
    "}\n",
    "surface_map = {\n",
    "    \"grass\": \"natural\",\n",
    "    \"grass \": \"natural\",\n",
    "    \"fieldturf\": \"artificial\",\n",
    "    \"sportturf\": \"artificial\",\n",
    "    \"matrixturf\": \"artificial\",\n",
    "    \"astroturf\": \"artificial\",\n",
    "    \"astroplay\": \"artificial\",\n",
    "    \"a_turf\": \"artificial\",\n",
    "}\n",
    "\n",
    "nfl_4th[\"roof\"] = nfl_4th[\"roof\"].map(roof_map)\n",
    "nfl_4th[\"surface\"] = nfl_4th[\"surface\"].map(surface_map)\n",
    "\n",
    "# fill missing surface and roof values with mode by stadium\n",
    "nfl_4th[\"roof\"] = nfl_4th.groupby(\"stadium\")[\"roof\"].transform(\n",
    "    lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else nfl_4th[\"roof\"].mode().iloc[0])\n",
    ")\n",
    "nfl_4th[\"surface\"] = nfl_4th.groupby(\"stadium\")[\"surface\"].transform(\n",
    "    lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else nfl_4th[\"surface\"].mode().iloc[0])\n",
    ")\n",
    "\n",
    "print(\"\\nWeather conditions overview:\")\n",
    "print(\" - Missing Temperature values by roof:\", nfl_4th[\"temp\"].isna().groupby(nfl_4th[\"roof\"]).sum().to_dict())\n",
    "print(\" - Mean Temperature by roof:\", nfl_4th.groupby(\"roof\")[\"temp\"].mean().to_dict())\n",
    "print(\"\\n - Missing Wind values by roof:\", nfl_4th[\"wind\"].isna().groupby(nfl_4th[\"roof\"]).sum().to_dict())\n",
    "print(\" - Mean Wind by roof:\", nfl_4th.groupby(\"roof\")[\"wind\"].mean().to_dict())\n",
    "\n",
    "# If roof is indoor and wind/temp is null, set wind and temp to 0 and 70 respectively\n",
    "nfl_4th.loc[nfl_4th[\"roof\"] == \"indoor\", \"wind\"] = nfl_4th.loc[nfl_4th[\"roof\"] == \"indoor\", \"wind\"].fillna(0)\n",
    "nfl_4th.loc[nfl_4th[\"roof\"] == \"indoor\", \"temp\"] = nfl_4th.loc[nfl_4th[\"roof\"] == \"indoor\", \"temp\"].fillna(70)\n",
    "\n",
    "nfl_4th[\"wind\"] = pd.cut(\n",
    "    nfl_4th[\"wind\"],\n",
    "    bins=[-1, 0, 5, 10, 15, np.inf],\n",
    "    labels=[\"Calm\", \"Light\", \"Moderate\", \"Windy\", \"VeryWindy\"]\n",
    ")\n",
    "\n",
    "nfl_4th[\"temp\"] = pd.cut(\n",
    "    nfl_4th[\"temp\"],\n",
    "    bins=[-np.inf, 32, 45, 60, 75, np.inf],\n",
    "    labels=[\"Frigid\", \"Cold\", \"Cool\", \"Mild\", \"Warm/Hot\"]\n",
    ")\n",
    "\n",
    "# fill missing wind and temp values with mode in outdoor stadiums\n",
    "nfl_4th[\"wind\"] = nfl_4th.groupby(\"roof\")[\"wind\"].transform(\n",
    "    lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else nfl_4th[\"wind\"].mode().iloc[0])\n",
    ")\n",
    "nfl_4th[\"temp\"] = nfl_4th.groupby(\"roof\")[\"temp\"].transform(\n",
    "    lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else nfl_4th[\"temp\"].mode().iloc[0])\n",
    ")\n",
    "\n",
    "# set ordinal categories\n",
    "nfl_4th[\"wind\"] = nfl_4th[\"wind\"].astype(\n",
    "    pd.api.types.CategoricalDtype(categories=[\"Calm\", \"Light\", \"Moderate\", \"Windy\", \"VeryWindy\"], ordered=True)\n",
    ")\n",
    "nfl_4th[\"temp\"] = nfl_4th[\"temp\"].astype(\n",
    "    pd.api.types.CategoricalDtype(categories=[\"Frigid\", \"Cold\", \"Cool\", \"Mild\", \"Warm/Hot\"], ordered=True)\n",
    ")\n",
    "\n",
    "print(\"\\n\\n =========== Variable Simplification ===========\\n\")\n",
    "print(\" - Type of stadium roofs:\", nfl_4th[\"roof\"].value_counts().to_dict())\n",
    "print(\" - Type of playing surfaces:\", nfl_4th[\"surface\"].value_counts().to_dict())\n",
    "print(\" - Wind conditions:\", nfl_4th[\"wind\"].value_counts().to_dict())\n",
    "print(\" - Temperature conditions:\", nfl_4th[\"temp\"].value_counts().to_dict())\n",
    "\n",
    "print(\"\\nWeather conditions overview:\")\n",
    "print(\" - Missing Temperature values by roof:\", nfl_4th[\"temp\"].isna().groupby(nfl_4th[\"roof\"]).sum().to_dict())\n",
    "print(\" - Missing Wind values by roof:\", nfl_4th[\"wind\"].isna().groupby(nfl_4th[\"roof\"]).sum().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c56b53",
   "metadata": {},
   "source": [
    "#### - Variables de NFL4th "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl4th_outputs = [\n",
    "    \"go_boost\",\n",
    "    \"first_down_prob\",\n",
    "    \"wp_fail\",\n",
    "    \"wp_succeed\",\n",
    "    \"go_wp\",\n",
    "    \"fg_make_prob\",\n",
    "    \"miss_fg_wp\",\n",
    "    \"make_fg_wp\",\n",
    "    \"fg_wp\",\n",
    "    \"punt_wp\",\n",
    "]\n",
    "\n",
    "print(\"\\nNFL4th outputs overview:\")\n",
    "print(\" - Nulls in NFL4th output variables:\\n\", nfl_4th[nfl4th_outputs].isna().sum())\n",
    "\n",
    "# fill first_down_prob and wp_succeed nulls with median\n",
    "nfl_4th[\"first_down_prob\"] = nfl_4th[\"first_down_prob\"].fillna(nfl_4th[\"first_down_prob\"].median())\n",
    "nfl_4th[\"wp_succeed\"] = nfl_4th[\"wp_succeed\"].fillna(nfl_4th[\"wp_succeed\"].median())\n",
    "\n",
    "print(\"\\n - Nulls in NFL4th output variables after imputation:\\n\", nfl_4th[nfl4th_outputs].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d92cd7",
   "metadata": {},
   "source": [
    "#### - EDA y resultados de la jugada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b50d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_helpers = [\n",
    "    \"ep\", \"wp\",\n",
    "]\n",
    "\n",
    "print(\"Nulls in EDA helper variables:\\n\", nfl_4th[eda_helpers].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_outcomes = [\"play_type_nfl\"]\n",
    "print(\"Nulls in observed outcome variables:\\n\", nfl_4th[observed_outcomes].isna().sum())\n",
    "\n",
    "# fill play_type_nfl where \"UNSPECIFIED\" with nfl_4th[\"play_type\"] with npl.where\n",
    "nfl_4th[\"play_type_nfl\"] = np.where(\n",
    "    nfl_4th[\"play_type_nfl\"] == \"UNSPECIFIED\",\n",
    "    nfl_4th[\"play_type\"].str.upper(),\n",
    "    nfl_4th[\"play_type_nfl\"]\n",
    ")\n",
    "\n",
    "# fill Nulls with conditions if contains (\"Punt\" -> \"PUNT\", \"Field Goal\" -> \"FIELD_GOAL\", \"Pass\" -> \"PASS\", \"Run\" -> \"RUN\")\n",
    "nfl_4th[\"play_type_nfl\"] = np.where(\n",
    "    nfl_4th[\"play_type_nfl\"].isna() & nfl_4th[\"desc\"].str.contains(\"Punt\", na=False),\n",
    "    \"PUNT\",\n",
    "    np.where(\n",
    "        nfl_4th[\"play_type_nfl\"].isna() & nfl_4th[\"desc\"].str.contains(\"Field Goal\", na=False),\n",
    "        \"FIELD_GOAL\",\n",
    "        np.where(\n",
    "            nfl_4th[\"play_type_nfl\"].isna() & nfl_4th[\"desc\"].str.contains(\"Pass\", na=False),\n",
    "            \"PASS\",\n",
    "            np.where(\n",
    "                nfl_4th[\"play_type_nfl\"].isna() & nfl_4th[\"desc\"].str.contains(\"Run\", na=False),\n",
    "                \"RUN\",\n",
    "                nfl_4th[\"play_type_nfl\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "nfl_4th[\"play_type_nfl\"] = nfl_4th[\"play_type_nfl\"].fillna(\"NO_PLAY\")\n",
    "\n",
    "play_map = {\n",
    "    \"PUNT\": \"PUNT\",\n",
    "    \"FIELD_GOAL\": \"FIELD_GOAL\",\n",
    "    \"PASS\": \"GO\",\n",
    "    \"RUSH\": \"GO\",\n",
    "    \"RUN\": \"GO\",\n",
    "    \"SACK\": \"GO\",\n",
    "    \"INTERCEPTION\": \"GO\",\n",
    "    \"NO_PLAY\": \"GO\",\n",
    "    \"FUMBLE_RECOVERED_BY_OPPONENT\": \"GO\",\n",
    "}\n",
    "nfl_4th[\"play_type_nfl\"] = nfl_4th[\"play_type_nfl\"].map(play_map)\n",
    "\n",
    "print(\"Nulls in play_type_nfl after imputation:\", nfl_4th[\"play_type_nfl\"].isna().sum())\n",
    "print(\"\\nPlay Type distributions:\\n\", nfl_4th[\"play_type_nfl\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288508a",
   "metadata": {},
   "source": [
    "## 03 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1A. Estimated FG distance (from yardline_100): snap 7 + endzone 10 = 17 yards\n",
    "nfl_4th = nfl_4th.copy()\n",
    "nfl_4th[\"est_fg_distance\"] = (nfl_4th[\"yardline_100\"] + 17).clip(lower=0)\n",
    "\n",
    "# --- 1B. Map actual play to triad (GO / FIELD_GOAL / PUNT)\n",
    "actual_map = {\n",
    "    \"GO\": \"GO\",\n",
    "    \"FIELD_GOAL\": \"FIELD_GOAL\",\n",
    "    \"PUNT\": \"PUNT\",\n",
    "}\n",
    "# If your column already uses exactly these three labels, this is a no-op:\n",
    "nfl_4th[\"actual_action\"] = nfl_4th[\"play_type_nfl\"].map(actual_map)\n",
    "\n",
    "# --- 1C. Masked action WPs (do NOT impute punt)\n",
    "nfl_4th[\"go_wp_m\"]   = nfl_4th[\"go_wp\"]\n",
    "nfl_4th[\"fg_wp_m\"]   = nfl_4th[\"fg_wp\"]\n",
    "nfl_4th[\"punt_wp_m\"] = nfl_4th[\"punt_wp\"]  # remains NaN if punt is not a viable action\n",
    "\n",
    "# How many actions are available per play?\n",
    "nfl_4th[\"n_actions_available\"] = (\n",
    "    nfl_4th[[\"go_wp_m\", \"fg_wp_m\", \"punt_wp_m\"]].notna().sum(axis=1)\n",
    ")\n",
    "\n",
    "# --- 1D. Model action (argmax among available actions)\n",
    "def pick_action(row):\n",
    "    vals = {\"GO\": row[\"go_wp_m\"], \"FIELD_GOAL\": row[\"fg_wp_m\"], \"PUNT\": row[\"punt_wp_m\"]}\n",
    "    # drop NaNs\n",
    "    vals = {k: v for k, v in vals.items() if pd.notna(v)}\n",
    "    if not vals:\n",
    "        return np.nan\n",
    "    return max(vals, key=vals.get)\n",
    "\n",
    "nfl_4th[\"model_action\"] = nfl_4th.apply(pick_action, axis=1)\n",
    "\n",
    "# --- 1E. Margin between top-1 and top-2 (closeness of the decision)\n",
    "def margin_top2(row):\n",
    "    xs = [row[\"go_wp_m\"], row[\"fg_wp_m\"], row[\"punt_wp_m\"]]\n",
    "    xs = [v for v in xs if pd.notna(v)]\n",
    "    if len(xs) < 2:\n",
    "        return np.nan\n",
    "    xs_sorted = sorted(xs, reverse=True)\n",
    "    return xs_sorted[0] - xs_sorted[1]\n",
    "\n",
    "nfl_4th[\"margin_top2\"] = nfl_4th.apply(margin_top2, axis=1)\n",
    "\n",
    "# --- 1F. Context bins for heatmaps / grouped plots\n",
    "nfl_4th[\"ytg_bin\"] = pd.cut(\n",
    "    nfl_4th[\"ydstogo\"],\n",
    "    bins=[0, 2, 5, 10, 100],\n",
    "    labels=[\"1-2\", \"3-5\", \"6-10\", \"11+\"],\n",
    "    right=True,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "nfl_4th[\"yardline_bin\"] = pd.cut(\n",
    "    nfl_4th[\"yardline_100\"],\n",
    "    bins=[0, 20, 40, 60, 80, 100],\n",
    "    labels=[\"RZ 0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"],\n",
    "    right=False,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "nfl_4th[\"clock_bucket_qtr\"] = pd.cut(\n",
    "    nfl_4th[\"quarter_seconds_remaining\"] / 60.0,\n",
    "    bins=[0, 2, 7, 15],\n",
    "    labels=[\"0-2 min\", \"2-7 min\", \"7-15 min\"],\n",
    "    right=True,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Environment is simplified:\n",
    "# roof: {'indoor','open_air'}, surface: {'natural','artificial'}\n",
    "# wind: {'Calm','Light','Moderate','Windy','VeryWindy'}\n",
    "# temp: {'Frigid','Cold','Cool','Mild','Warm/Hot'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130231e",
   "metadata": {},
   "source": [
    "#### - Volumetría y disponibilidad de jugadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4740af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique plays:\", len(nfl_4th))\n",
    "print(\"Triad counts (actual):\\n\", nfl_4th[\"actual_action\"].value_counts(dropna=False))\n",
    "print(\"\\nHow many actions available per play:\")\n",
    "print(nfl_4th[\"n_actions_available\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nPunt WP NA rate (means punt not viable):\",\n",
    "      nfl_4th[\"punt_wp_m\"].isna().mean())\n",
    "\n",
    "\n",
    "palette_actions = {\"GO\": colors[\"go\"], \"FIELD_GOAL\": colors[\"fg\"], \"PUNT\": colors[\"punt\"]}\n",
    "\n",
    "# =========================================================\n",
    "# PLOT A — Actual 4th-Down Decisions (Counts + % labels)\n",
    "# =========================================================\n",
    "order = [\"PUNT\", \"FIELD_GOAL\", \"GO\"]\n",
    "counts = (nfl_4th[\"actual_action\"]\n",
    "          .value_counts()\n",
    "          .reindex(order)\n",
    "          .rename_axis(\"action\")\n",
    "          .reset_index(name=\"count\"))\n",
    "counts[\"share\"] = counts[\"count\"] / counts[\"count\"].sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.95))\n",
    "sns.barplot(\n",
    "    data=counts, x=\"action\", y=\"count\", order=order,\n",
    "    palette=[palette_actions[a] for a in order], ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Actual 4th-Down Decisions (2014–2023)\", fontsize=8)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Plays\")\n",
    "\n",
    "# annotate percentages above bars (small, tidy)\n",
    "offset = counts[\"count\"].max() * 0.01\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    if h > 0:\n",
    "        ax.annotate(f\"{(h / counts['count'].sum())*100:.1f}%\",\n",
    "                    (p.get_x() + p.get_width()/2, h + offset),\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=7, xytext=(0, 2),\n",
    "                    textcoords=\"offset points\")\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"actual_decisions_counts.png\"))\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# PUNT UNAVAILABLE — 1D GRADIENT ACROSS YARDLINE_100 (DROP YTG AXIS)\n",
    "# =========================================================\n",
    "df = nfl_4th.copy()\n",
    "df[\"punt_unavailable\"] = df[\"punt_wp\"].isna().astype(int)\n",
    "\n",
    "# Share by exact yardline_100 (0–100). Cast to int yards for a smooth gradient strip.\n",
    "yl_int = df[\"yardline_100\"].round().astype(int).clip(0, 100)\n",
    "share_by_yard = (df.assign(yl=yl_int)\n",
    "                   .groupby(\"yl\")[\"punt_unavailable\"]\n",
    "                   .mean()\n",
    "                   .reindex(range(0, 101)))  # ensure full 0..100 coverage\n",
    "\n",
    "# Make a 1×101 \"strip\" for a single-axis heatmap\n",
    "grad_df = pd.DataFrame([share_by_yard.values], index=[\"share\"], columns=range(0, 101))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.30))\n",
    "sns.heatmap(\n",
    "    grad_df, cmap=\"YlOrRd\", vmin=0, vmax=1, cbar_kws={\"label\": \"Share punt unavailable\"},\n",
    "    ax=ax, yticklabels=False, xticklabels=10  # show every 10 yards\n",
    ")\n",
    "\n",
    "ax.set_title(\"Where is Punt Unavailable? (by field position)\", fontsize=8)\n",
    "ax.set_xlabel(\"Field position (yards to EZ)\")\n",
    "ax.set_ylabel(\"\")\n",
    "\n",
    "# Clean up spines for a strip look\n",
    "for spine in [\"left\", \"right\", \"top\", \"bottom\"]:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"punt_unavailable_yardline_gradient.png\"))\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# PLOT B — Availability of Actions (Donut)\n",
    "# =========================================================\n",
    "avail = (nfl_4th[\"n_actions_available\"]\n",
    "         .value_counts()\n",
    "         .reindex([2, 3])\n",
    "         .rename_axis(\"n_actions\")\n",
    "         .reset_index(name=\"count\"))\n",
    "avail[\"share\"] = avail[\"count\"] / avail[\"count\"].sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.95))\n",
    "vals = avail[\"count\"].values\n",
    "labels = [f\"{n} actions\\n{c:,} ({s*100:.1f}%)\"\n",
    "          for n, c, s in zip(avail[\"n_actions\"], avail[\"count\"], avail[\"share\"])]\n",
    "\n",
    "# neutral greys to keep action colors reserved for actions\n",
    "wedge_colors = [\"#9ea3a8\", \"#d2d6da\"]\n",
    "wedges, _ = ax.pie(\n",
    "    vals, startangle=90, counterclock=False,\n",
    "    wedgeprops=dict(width=0.35), colors=wedge_colors\n",
    ")\n",
    "# white center for the donut\n",
    "centre_circle = plt.Circle((0, 0), 0.65, fc=\"white\")\n",
    "ax.add_artist(centre_circle)\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "ax.set_title(\"Action Availability on 4th Down\", fontsize=8)\n",
    "ax.legend(wedges, labels, loc=\"center\", bbox_to_anchor=(0.5, -0.15),\n",
    "          ncol=1, frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"availability_donut.png\"))\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# PLOT D — Actual Choice by Availability (Shares, colored by action)\n",
    "# =========================================================\n",
    "subset = nfl_4th[nfl_4th[\"n_actions_available\"].isin([2, 3])].copy()\n",
    "prop = (subset\n",
    "        .groupby([\"n_actions_available\", \"actual_action\"], group_keys=False)\n",
    "        .size()\n",
    "        .groupby(level=0, group_keys=False)\n",
    "        .apply(lambda s: s / s.sum())\n",
    "        .rename(\"prop\")\n",
    "        .reset_index())\n",
    "\n",
    "order_actions = [\"GO\", \"FIELD_GOAL\", \"PUNT\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.95))\n",
    "sns.barplot(\n",
    "    data=prop, x=\"n_actions_available\", y=\"prop\", hue=\"actual_action\",\n",
    "    hue_order=order_actions, palette=palette_actions, ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Actual Choice by Availability\", fontsize=8)\n",
    "ax.set_xlabel(\"Number of actions available\")\n",
    "ax.set_ylabel(\"Share\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(title=\"Action\", loc=\"upper right\", frameon=False)\n",
    "\n",
    "# annotate bars with share %\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    if h > 0.01:\n",
    "        ax.annotate(f\"{h*100:.1f}%\",\n",
    "                    (p.get_x() + p.get_width()/2, h),\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=7, xytext=(0, 2),\n",
    "                    textcoords=\"offset points\")\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"actual_choice_by_availability.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc04f4",
   "metadata": {},
   "source": [
    "- Más de 40,000 jugadas (57% Punt, 23% FG Attempt, 18% GO)\n",
    "- En el 74% de los escenarios se tienen 3 posibles jugadas disponibles, distancia mínima de despeje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1fa46",
   "metadata": {},
   "source": [
    "#### - Acuerdo entre las decisiones del coach y el diferencial de probabilidad de victoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pd.crosstab(nfl_4th[\"actual_action\"], nfl_4th[\"model_action\"], normalize=\"index\").round(3)\n",
    "display(conf)\n",
    "\n",
    "# Raw counts too:\n",
    "display(pd.crosstab(nfl_4th[\"actual_action\"], nfl_4th[\"model_action\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe6625",
   "metadata": {},
   "source": [
    "- Overall agreement ~67.8% (27,546 / 40,655). Diagonal by class: GO 78.5%, FG 68.2%, PUNT 64.1% → the model aligns best on GO decisions and least on punts.\n",
    "\n",
    "- Model is much more aggressive than coaches.\n",
    "    - Predicted GO ~41.7% (16,946) vs actual GO ~18.7% (7,591).\n",
    "    - Predicted PUNT ~39.1% (15,894) vs actual PUNT ~57.7% (23,455).\n",
    "    - Predicted FG ~19.2% (7,815) vs actual FG ~23.7% (9,609).\n",
    "\n",
    "- Where disagreements cluster:\n",
    "    - When coaches punted, the model said GO in 34.2% of those cases (8,021 / 23,455).\n",
    "    - When coaches kicked FG, the model said GO in 30.9% (2,967 / 9,609).\n",
    "    - When coaches went for it, the model still preferred FG or PUNT in ~21.5% (1,633 / 7,591).\n",
    "\n",
    "- Asymmetry is clear: Coaches favor PUNT far more than the model (64.1% agreement on punts, but 34.2% of punts are flagged as GO by the model), while GO has the highest alignment (78.5%) — suggesting many “go” situations are robust, but a large set of “punt” situations are borderline where the model sees value in aggression.\n",
    "\n",
    "- FG vs GO boundary is tight: ~31% of coach FGs are flagged GO by the model, consistent with a model that values expected WP gains from conversion over 3 points in many mid-range spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which action is best by WP?\n",
    "best_action = nfl_4th[[\"go_wp_m\",\"fg_wp_m\",\"punt_wp_m\"]].idxmax(axis=1)\n",
    "best_action = best_action.replace({\"go_wp_m\":\"GO\",\"fg_wp_m\":\"FIELD_GOAL\",\"punt_wp_m\":\"PUNT\"})\n",
    "nfl_4th[\"best_action\"] = best_action\n",
    "\n",
    "pivot = (nfl_4th\n",
    "         .pivot_table(index=\"yardline_bin\", columns=\"ytg_bin\",\n",
    "                      values=\"best_action\", aggfunc=lambda s: s.value_counts(normalize=True).idxmax()))\n",
    "display(pivot)\n",
    "\n",
    "# Also show % of best being FG/PUNT/GO per bin\n",
    "share = (nfl_4th\n",
    "         .groupby([\"yardline_bin\",\"ytg_bin\",\"best_action\"], group_keys=False)\n",
    "         .size()\n",
    "         .groupby(level=[0,1], group_keys=False).apply(lambda s: s / s.sum())\n",
    "         .rename(\"share\")\n",
    "         .reset_index())\n",
    "display(share.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2145b3",
   "metadata": {},
   "source": [
    "No punts in the red zone (0–20 yds to go to the EZ).\n",
    "Across all YTG bins in RZ 0–20, the model never recommends punt (share = 0.000).\n",
    "\n",
    "Short yardage (≤5 yds): “GO” almost everywhere.\n",
    "\n",
    "RZ 0–20, 1–2 yds: GO 82.6%, FG 17.4%.\n",
    "\n",
    "RZ 0–20, 3–5 yds: essentially a coin flip (FG 51.2%, GO 48.8%).\n",
    "\n",
    "20–40 and 40–100: the table shows GO for both 1–2 and 3–5 across all non-RZ bins.\n",
    "\n",
    "Medium yardage (6–10 yds): context flips the action.\n",
    "\n",
    "RZ 0–20: FG is preferred (74.9% FG, 25.1% GO).\n",
    "\n",
    "Outside RZ (20–100): PUNT becomes preferred (per the grid: 40–60, 60–80, 80–100 all say PUNT; 20–40 says FG, see note below).\n",
    "\n",
    "Long yardage (11+ yds): FG only in the RZ; otherwise PUNT.\n",
    "\n",
    "RZ 0–20: FG 92.9%.\n",
    "\n",
    "20–100: PUNT is recommended in all non-RZ bins (per the grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43390b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_margin = (nfl_4th\n",
    "              .groupby([\"yardline_bin\",\"ytg_bin\"])[\"margin_top2\"]\n",
    "              .mean()\n",
    "              .unstack(1)\n",
    "              .round(3))\n",
    "display(agg_margin)\n",
    "\n",
    "print(\"Overall median margin:\", nfl_4th[\"margin_top2\"].median().round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5fb3f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share of best action by wind category\n",
    "wind_best = (nfl_4th\n",
    "             .groupby([\"wind\",\"best_action\"], group_keys=True)\n",
    "             .size()\n",
    "             .groupby(level=0, group_keys=False).apply(lambda s: s / s.sum())\n",
    "             .rename(\"share\")\n",
    "             .reset_index())\n",
    "\n",
    "# Share of best action by temp category\n",
    "temp_best = (nfl_4th\n",
    "             .groupby([\"temp\",\"best_action\"], group_keys=False)\n",
    "             .size()\n",
    "             .groupby(level=0, group_keys=False).apply(lambda s: s / s.sum())\n",
    "             .rename(\"share\")\n",
    "             .reset_index())\n",
    "\n",
    "display(wind_best.head(10))\n",
    "display(temp_best.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689abfb",
   "metadata": {},
   "source": [
    "Wind (Calm → Light/Moderate shown)\n",
    "\n",
    "GO share drops as wind picks up: 44.4% (Calm) → ~41% (Light/Moderate).\n",
    "\n",
    "PUNT share rises in the same shift: 35.9% → ~40%.\n",
    "\n",
    "FG share stays ~flat (~18–20%).\n",
    "Interpretation: with more wind the policy nudges from GO → PUNT, not toward FG. (Makes sense: wind hurts the “go” path’s passing more broadly; long FGs are also wind-sensitive, so the safer default becomes punt.)\n",
    "\n",
    "Temperature (Frigid → Cool → Mild shown)\n",
    "\n",
    "Shares barely move: GO ~40%, PUNT ~39–40%, FG ~19–21%.\n",
    "\n",
    "Slight FG uptick in Frigid (20.7%) vs Mild (19.2%), but differences are small.\n",
    "Interpretation: unconditional temperature effects are weak; field position & YTG dominate the decision. Any temp effect likely appears at longer FG distances only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin estimated FG distance\n",
    "nfl_4th[\"fg_dist_bin\"] = pd.cut(\n",
    "    nfl_4th[\"est_fg_distance\"],\n",
    "    bins=[0,30,35,40,45,50,55,60,80],\n",
    "    labels=[\"<30\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60+\"],\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Share of MODEL recommending FG by distance\n",
    "fg_share_model = (nfl_4th\n",
    "                  .groupby(\"fg_dist_bin\")[\"model_action\"]\n",
    "                  .apply(lambda s: (s==\"FIELD_GOAL\").mean())\n",
    "                  .round(3))\n",
    "\n",
    "# Share of ACTUAL FG by distance\n",
    "fg_share_actual = (nfl_4th\n",
    "                   .groupby(\"fg_dist_bin\")[\"actual_action\"]\n",
    "                   .apply(lambda s: (s==\"FIELD_GOAL\").mean())\n",
    "                   .round(3))\n",
    "\n",
    "# Average FG make probability by distance (from nfl4th)\n",
    "fg_make_by_dist = nfl_4th.groupby(\"fg_dist_bin\")[\"fg_make_prob\"].mean().round(3)\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    \"fg_model_share\": fg_share_model,\n",
    "    \"fg_actual_share\": fg_share_actual,\n",
    "    \"fg_make_prob_avg\": fg_make_by_dist\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a6a6b",
   "metadata": {},
   "source": [
    "Make prob drops cleanly with distance (0.98 → 0.03 from <30 to 60+). Calibration looks monotonic and sensible.\n",
    "\n",
    "Coaches kick more often than the model across standard ranges (<55 yds).\n",
    "Biggest gaps:\n",
    "\n",
    "40–44 yds: model 0.648 vs actual 0.793 (−0.145)\n",
    "\n",
    "45–49 yds: 0.582 vs 0.742 (−0.160)\n",
    "\n",
    "35–39 yds: 0.700 vs 0.817 (−0.117)\n",
    "Interpretation: in the mid-range, the model more often prefers GO or PUNT because the WP gain from a TD (or field position) can beat the 3 points, even with decent make rates.\n",
    "\n",
    "Short range (<30): coaches kick ~69% vs model ~44%. The model is more aggressive here—preferring GO on short yardage despite near-automatic FGs.\n",
    "\n",
    "Very long range (55–59): model kicks more than coaches (0.182 vs 0.085). Likely end-of-half/game spots where a long FG can rival the value of a punt.\n",
    "\n",
    "60+: both essentially avoid FGs (near zero), aligned with the tiny make prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-minute drill flag (per half)\n",
    "nfl_4th[\"two_min_drill\"] = (nfl_4th[\"half_seconds_remaining\"] <= 120).astype(\"int8\")\n",
    "\n",
    "# Action shares by quarter and two-minute\n",
    "late_behavior = (nfl_4th\n",
    "                 .groupby([\"qtr\",\"two_min_drill\"])[\"model_action\"]\n",
    "                 .value_counts(normalize=True)\n",
    "                 .rename(\"share\")\n",
    "                 .reset_index())\n",
    "\n",
    "display(late_behavior.head(20))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 3 — Actual Decision Mix inside Two-Minute Warning of a Close Game\n",
    "# =========================================================\n",
    "ctx = nfl_4th.copy()\n",
    "ctx[\"two_minute\"] = (ctx[\"half_seconds_remaining\"] <= 120)\n",
    "ctx[\"close_game\"] = (ctx[\"score_differential\"].abs() <= 3)\n",
    "ctx[\"one_possession_game\"] = (ctx[\"score_differential\"].abs() <= 8)\n",
    "ctx[\"context\"] = np.where(ctx[\"two_minute\"] & ctx[\"close_game\"],\n",
    "                          \"2-min left (±3 pts)\",\n",
    "                          np.where(ctx[\"two_minute\"] & ctx[\"one_possession_game\"],\n",
    "                                   \"2-min left (±8 pts)\",\n",
    "                                   \"Other\"))\n",
    "\n",
    "order_actions = [\"GO\", \"FIELD_GOAL\", \"PUNT\"]\n",
    "palette_actions = {\"GO\": colors[\"go\"], \"FIELD_GOAL\": colors[\"fg\"], \"PUNT\": colors[\"punt\"]}\n",
    "\n",
    "# Proportions by context\n",
    "prop_ctx = (ctx.groupby([\"context\",\"actual_action\"], group_keys=False)\n",
    "              .size()\n",
    "              .groupby(level=0, group_keys=False)\n",
    "              .apply(lambda s: s / s.sum())\n",
    "              .rename(\"share\")\n",
    "              .reset_index())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.95))\n",
    "sns.barplot(\n",
    "    data=prop_ctx, x=\"context\", y=\"share\", hue=\"actual_action\",\n",
    "    hue_order=order_actions, palette=palette_actions, ax=ax\n",
    ")\n",
    "\n",
    "# Axes & legend (minimalist)\n",
    "ax.set_title(\"Actual Decision Mix\\nInside Two-Minute Left of a Close Game\", fontsize=8)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Share\")\n",
    "ax.set_ylim(0, 1)\n",
    "leg = ax.legend(title=\"Actual action\", frameon=False, loc=\"upper right\")\n",
    "if leg:\n",
    "    try:\n",
    "        # Align legend entries to the left (best-effort; seaborn/mpl internals can vary)\n",
    "        leg._legend_box.align = \"left\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Bar annotations (percents), small and tidy\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    if h > 0.02:\n",
    "        ax.annotate(f\"{h*100:.0f}%\",\n",
    "                    (p.get_x() + p.get_width()/2, h),\n",
    "                    ha=\"center\", va=\"bottom\",\n",
    "                    fontsize=7, xytext=(0, 2),\n",
    "                    textcoords=\"offset points\")\n",
    "\n",
    "fig.tight_layout()\n",
    "if SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"decision_mix_2min.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74eb8ad",
   "metadata": {},
   "source": [
    "Early game (Q1–Q3, outside 2-min): model slightly prefers PUNT ≈ 41–46% over GO ≈ 39–40%, with FG ≈ 13–20%.\n",
    "\n",
    "Q1 (non–2-min): PUNT 46.4%, GO 40.3%, FG 13.3%.\n",
    "\n",
    "Q2 (non–2-min): PUNT 40.9%, GO 38.7%, FG 20.3%.\n",
    "\n",
    "Q3 (non–2-min): PUNT 41.5%, GO 40.5%, FG 18.0%.\n",
    "\n",
    "Halftime 2-minute (Q2, 2-min): the model shifts toward FG and away from GO:\n",
    "\n",
    "Q2 (2-min): FG 31.5% (↑ ~+11 pts vs non–2-min), GO 30.0% (↓ ~−8.7 pts), PUNT 38.6% (slight ↓).\n",
    "\n",
    "Interpretation: with limited time before halftime, taking the guaranteed scoring chance rises in value.\n",
    "\n",
    "Late game (Q4, outside 2-min): aggression ramps up:\n",
    "\n",
    "Q4 (non–2-min): GO 44.7% (highest outside 2-min), PUNT 34.3%, FG 21.0%.\n",
    "\n",
    "Endgame (Q4, 2-min): big swing to GO:\n",
    "\n",
    "Q4 (2-min): GO 65.0% (↑ ~+20 pts vs non–2-min), PUNT 16.2% (↓ ~−18 pts), FG 18.7% (−2.3 pts).\n",
    "\n",
    "Interpretation: with the game on the line, the model prioritizes drive survival over field position or three points.\n",
    "\n",
    "OT (qtr=5): model leans conservative but balanced:\n",
    "\n",
    "PUNT 42.5%, GO 34.8%, FG ~22.7% (by remainder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement flag\n",
    "nfl_4th[\"agree\"] = (nfl_4th[\"actual_action\"] == nfl_4th[\"model_action\"]).astype(\"int8\")\n",
    "print(\"Overall agreement:\", nfl_4th[\"agree\"].mean().round(3))\n",
    "\n",
    "# Agreement by context (field position / ytg)\n",
    "agree_table = (nfl_4th\n",
    "               .groupby([\"yardline_bin\",\"ytg_bin\"])[\"agree\"]\n",
    "               .mean()\n",
    "               .unstack(1)\n",
    "               .round(3))\n",
    "display(agree_table)\n",
    "\n",
    "# Agreement by environment\n",
    "agree_env = (nfl_4th\n",
    "             .groupby([\"wind\",\"temp\"])[\"agree\"]\n",
    "             .mean()\n",
    "             .unstack(0)\n",
    "             .round(3))\n",
    "display(agree_env)\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 1 — FG make probability vs. distance (15–65 yd)\n",
    "# =========================================================\n",
    "df = nfl_4th.copy()\n",
    "df[\"fg_dist_yd\"] = (df[\"yardline_100\"] + 17).clip(lower=0)\n",
    "s = df.loc[:, [\"fg_dist_yd\", \"fg_make_prob\"]].dropna().copy()\n",
    "s = s.query(\"fg_dist_yd >= 15 and fg_dist_yd <= 65\").copy()\n",
    "\n",
    "# GLM(logit) fit\n",
    "eps = 1e-6\n",
    "y_fg = s[\"fg_make_prob\"].clip(eps, 1 - eps)\n",
    "X_fg = sm.add_constant(s[\"fg_dist_yd\"])\n",
    "glm_fg = sm.GLM(y_fg, X_fg, family=sm.families.Binomial()).fit()\n",
    "\n",
    "x_pred_fg = np.linspace(15, 65, 400)\n",
    "y_pred_fg = glm_fg.predict(sm.add_constant(x_pred_fg))\n",
    "\n",
    "fig, ax_fg = plt.subplots(figsize=(WIDTH, WIDTH*0.95))\n",
    "ax_fg.hexbin(\n",
    "    s[\"fg_dist_yd\"], s[\"fg_make_prob\"],\n",
    "    gridsize=40, cmap=\"Oranges\",\n",
    "    extent=(15, 65, 0, 1), mincnt=1\n",
    ")\n",
    "ax_fg.plot(x_pred_fg, y_pred_fg, color=colors[\"fg\"], lw=1.6)\n",
    "\n",
    "ax_fg.set_title(\"FG make probability vs. distance\", fontsize=8)\n",
    "ax_fg.set_xlabel(\"FG distance (yd)\")\n",
    "ax_fg.set_ylabel(\"FG conversion %\")\n",
    "ax_fg.set_ylim(0, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "if SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"fg_prob_vs_distance.png\"))\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 2 — 4th-down conversion probability vs. yards to go (0–15)\n",
    "# =========================================================\n",
    "s = nfl_4th.loc[:, [\"ydstogo\", \"first_down_prob\"]].dropna().copy()\n",
    "s = s.query(\"ydstogo >= 0 and ydstogo <= 15\").copy()\n",
    "\n",
    "# GLM(logit) fit\n",
    "eps = 1e-6\n",
    "y_go = s[\"first_down_prob\"].clip(eps, 1 - eps)\n",
    "X_go = sm.add_constant(s[\"ydstogo\"])\n",
    "glm_go = sm.GLM(y_go, X_go, family=sm.families.Binomial()).fit()\n",
    "\n",
    "x_pred_go = np.linspace(0, 15, 400)\n",
    "y_pred_go = glm_go.predict(sm.add_constant(x_pred_go))\n",
    "\n",
    "fig, ax_go = plt.subplots(figsize=(WIDTH, WIDTH*0.95))\n",
    "ax_go.hexbin(\n",
    "    s[\"ydstogo\"], s[\"first_down_prob\"],\n",
    "    gridsize=40, cmap=\"Purples\",\n",
    "    extent=(0, 15, 0, 1), mincnt=1\n",
    ")\n",
    "ax_go.plot(x_pred_go, y_pred_go, color=colors[\"go\"], lw=1.6)\n",
    "\n",
    "ax_go.set_title(\"4th-down conversion probability vs. yards to go\", fontsize=8)\n",
    "ax_go.set_xlabel(\"Yards to go\")\n",
    "ax_go.set_ylabel(\"4th Down Conversion\")\n",
    "ax_go.set_ylim(0, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "if SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"go_prob_vs_ytg.png\"))\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 3 — Recommended Action by Weather (Wind & Temperature)\n",
    "# =========================================================\n",
    "# Compute model_action if not present (argmax of available WPs)\n",
    "def _pick_action(row):\n",
    "    vals = {\"GO\": row.get(\"go_wp\"), \"FIELD_GOAL\": row.get(\"fg_wp\"), \"PUNT\": row.get(\"punt_wp\")}\n",
    "    vals = {k: v for k, v in vals.items() if pd.notna(v)}\n",
    "    return max(vals, key=vals.get) if vals else np.nan\n",
    "\n",
    "if \"model_action\" not in nfl_4th.columns:\n",
    "    nfl_4th[\"model_action\"] = nfl_4th.apply(_pick_action, axis=1)\n",
    "\n",
    "# Expected category orders\n",
    "wind_order = [\"Calm\", \"Light\", \"Moderate\", \"Windy\", \"VeryWindy\"]\n",
    "temp_order = [\"Frigid\", \"Cold\", \"Cool\", \"Mild\", \"Warm/Hot\"]\n",
    "act_order  = [\"GO\", \"FIELD_GOAL\", \"PUNT\"]\n",
    "\n",
    "wind_share = (nfl_4th\n",
    "              .groupby([\"wind\", \"model_action\"], group_keys=False)\n",
    "              .size()\n",
    "              .groupby(level=0, group_keys=False)\n",
    "              .apply(lambda s: s / s.sum())\n",
    "              .rename(\"share\")\n",
    "              .reset_index())\n",
    "wind_share[\"wind\"] = pd.Categorical(wind_share[\"wind\"], categories=wind_order, ordered=True)\n",
    "\n",
    "temp_share = (nfl_4th\n",
    "              .groupby([\"temp\", \"model_action\"], group_keys=False)\n",
    "              .size()\n",
    "              .groupby(level=0, group_keys=False)\n",
    "              .apply(lambda s: s / s.sum())\n",
    "              .rename(\"share\")\n",
    "              .reset_index())\n",
    "temp_share[\"temp\"] = pd.Categorical(temp_share[\"temp\"], categories=temp_order, ordered=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(WIDTH*1.8, WIDTH*0.95), sharey=True)\n",
    "\n",
    "sns.barplot(\n",
    "    data=wind_share, x=\"wind\", y=\"share\", hue=\"model_action\",\n",
    "    hue_order=act_order, palette=palette_actions, ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Recommended Action by Wind\", fontsize=8)\n",
    "axes[0].set_xlabel(\"Wind category\")\n",
    "axes[0].set_ylabel(\"Share\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend_.remove()\n",
    "\n",
    "sns.barplot(\n",
    "    data=temp_share, x=\"temp\", y=\"share\", hue=\"model_action\",\n",
    "    hue_order=act_order, palette=palette_actions, ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Recommended Action by Temperature\", fontsize=8)\n",
    "axes[1].set_xlabel(\"Temperature category\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].legend(title=\"Model action\", loc=\"upper right\", frameon=False)\n",
    "\n",
    "# annotate bars\n",
    "for ax in axes:\n",
    "    for p in ax.patches:\n",
    "        h = p.get_height()\n",
    "        if h > 0.03:\n",
    "            ax.annotate(f\"{h*100:.0f}%\",\n",
    "                        (p.get_x() + p.get_width()/2, h),\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=7,\n",
    "                        xytext=(0, 2), textcoords=\"offset points\")\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"recommended_action_by_weather.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dcd69d",
   "metadata": {},
   "source": [
    "Field position × YTG (agreement)\n",
    "\n",
    "Obvious spots = high agreement.\n",
    "\n",
    "Red zone, long YTG (11+) → 0.914 (FG for both).\n",
    "\n",
    "Own territory, long YTG (60–100 & 11+) → 0.925–0.935 (PUNT for both).\n",
    "\n",
    "Own territory, 6–10 → 0.810–0.818 (mostly PUNT).\n",
    "\n",
    "Short yardage outside the red zone = big disagreement.\n",
    "\n",
    "60–80, 1–2 → 0.279 and 80–100, 1–2 → 0.173 (coaches punt a lot; model wants GO).\n",
    "\n",
    "Midfield 40–60, 3–5 → 0.395 (borderline; model more aggressive).\n",
    "\n",
    "Red zone, 1–2 has only moderate agreement (0.660) → coaches kick FGs more than the model would in prime GO situations.\n",
    "\n",
    "Takeaway: disagreement concentrates in short YTG when you’re not yet in the red zone. Everywhere else, choices are pretty aligned.\n",
    "\n",
    "Environment (temp × wind → agreement)\n",
    "\n",
    "Agreement is remarkably flat by weather: mostly 0.65–0.73 across cells.\n",
    "\n",
    "No systematic pattern big enough to rival field position/YTG effects.\n",
    "\n",
    "Takeaway: for agreement, weather ≪ situation. (Weather still matters for which action is best in certain slices, but it doesn’t drive coach–model gaps nearly as much.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a957197",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "\n",
    "summary[\"n_plays\"] = len(nfl_4th)\n",
    "summary[\"action_counts\"] = nfl_4th[\"actual_action\"].value_counts().to_dict()\n",
    "summary[\"overall_agreement\"] = round(nfl_4th[\"agree\"].mean(), 3)\n",
    "\n",
    "# Where model says FG most often\n",
    "fg_by_dist = (nfl_4th.groupby(\"fg_dist_bin\")[\"model_action\"]\n",
    "              .apply(lambda s: (s==\"FIELD_GOAL\").mean()))\n",
    "summary[\"fg_peak_bin\"] = fg_by_dist.idxmax()\n",
    "summary[\"fg_peak_share\"] = round(fg_by_dist.max(), 3)\n",
    "\n",
    "# Where decisions are closest\n",
    "summary[\"closest_bins\"] = (nfl_4th.groupby([\"yardline_bin\",\"ytg_bin\"])[\"margin_top2\"]\n",
    "                           .mean().sort_values().head(5))\n",
    "\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39640b3",
   "metadata": {},
   "source": [
    "Scale & class mix. 40,655 fourth‐down plays: PUNT 57.7%, FG 23.7%, GO 18.7% → strong class imbalance toward punts (use class-aware metrics later).\n",
    "\n",
    "Coach–model alignment. Overall agreement = 0.678. That’s decent but leaves meaningful room for improvement—good for a “coach’s agreement” KPI.\n",
    "\n",
    "Where the model likes FGs most. Peak FG recommendation is at 30–34 yds (estimated distance), with FG share = 0.703. This is your “automatic FG” zone for the policy narrative.\n",
    "\n",
    "True gray zones (smallest WP margin between top-1 and top-2 actions).\n",
    "The tightest decisions (margin ≈ 0.008–0.015) cluster in:\n",
    "\n",
    "Own territory, 3–5 yds to go: 80–100 and 60–80 bins\n",
    "\n",
    "Midfield, 3–5 yds and 6–10 yds: 40–60 bin\n",
    "\n",
    "Opp side but not red zone, 3–5 yds: 20–40 bin\n",
    "Translation: the 3–5 yds band is the universal knife-edge, with midfield 6–10 also borderline. Great places to surface uncertainty in the UI (“coin-flip” badges, show margin).\n",
    "\n",
    "Storyline to use. “The model partitions the field cleanly (short-yardage → GO; red-zone long yardage → FG; deep/long → PUNT), but short-yardage outside the red zone creates consistent borderline trade-offs between aggression and field position.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf43b8",
   "metadata": {},
   "source": [
    "### Variables objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10abaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistent action palette from prior cell\n",
    "palette_actions = {\"GO\": colors[\"go\"], \"FIELD_GOAL\": colors[\"fg\"], \"PUNT\": colors[\"punt\"]}\n",
    "act_order = [\"GO\",\"FIELD_GOAL\",\"PUNT\"]\n",
    "\n",
    "df = nfl_4th.copy()\n",
    "\n",
    "# Availability flags (punt_wp is NA when punt not viable in nfl4th)\n",
    "df[\"avail_go\"] = np.isfinite(df[\"go_wp\"])\n",
    "df[\"avail_fg\"] = np.isfinite(df[\"fg_wp\"])\n",
    "df[\"avail_punt\"] = np.isfinite(df[\"punt_wp\"])\n",
    "\n",
    "# Estimated FG distance\n",
    "df[\"fg_dist_yd\"] = (df[\"yardline_100\"] + 17).clip(lower=0)\n",
    "\n",
    "# Model policy targets from nfl4th outputs (teacher)\n",
    "wp_cols = [\"go_wp\", \"fg_wp\", \"punt_wp\"]\n",
    "_wp = df[wp_cols].copy().where(np.isfinite(df[wp_cols]), -np.inf)\n",
    "\n",
    "# y_opt_action (argmax of WPs over available actions)\n",
    "opt_idx = _wp.to_numpy().argmax(axis=1)\n",
    "df[\"y_opt_action\"] = np.take([\"go_wp\", \"fg_wp\", \"punt_wp\"], opt_idx)\n",
    "df[\"y_opt_action\"] = df[\"y_opt_action\"].map({\"go_wp\":\"GO\",\"fg_wp\":\"FIELD_GOAL\",\"punt_wp\":\"PUNT\"})\n",
    "\n",
    "# y_confidence = top1 - top2 among available actions\n",
    "sorted_wp = np.sort(_wp.to_numpy(), axis=1)\n",
    "df[\"y_confidence\"] = (sorted_wp[:, -1] - sorted_wp[:, -2]).astype(float)\n",
    "\n",
    "# actual_action from cleaned label\n",
    "df[\"actual_action\"] = pd.Categorical(df[\"play_type_nfl\"], categories=act_order)\n",
    "\n",
    "# y_wp_regret = best(WP) - WP(actual_action)\n",
    "def _wp_of_action_row(row):\n",
    "    if row[\"actual_action\"] == \"GO\":\n",
    "        return row[\"go_wp\"]\n",
    "    if row[\"actual_action\"] == \"FIELD_GOAL\":\n",
    "        return row[\"fg_wp\"]\n",
    "    if row[\"actual_action\"] == \"PUNT\":\n",
    "        return row[\"punt_wp\"]\n",
    "    return np.nan\n",
    "\n",
    "df[\"wp_actual\"] = df.apply(_wp_of_action_row, axis=1)\n",
    "df[\"wp_best\"] = _wp.max(axis=1)\n",
    "df[\"y_wp_regret\"] = (df[\"wp_best\"] - df[\"wp_actual\"]).astype(float)\n",
    "\n",
    "# Binning for field position / yards-to-go\n",
    "def _ytg_bin(y):\n",
    "    if y <= 2: return \"1-2\"\n",
    "    if y <= 5: return \"3-5\"\n",
    "    if y <= 10: return \"6-10\"\n",
    "    return \"11+\"\n",
    "\n",
    "def _yard_bin(yl):\n",
    "    if yl <= 20:  return \"RZ 0-20\"\n",
    "    if yl <= 40:  return \"20-40\"\n",
    "    if yl <= 60:  return \"40-60\"\n",
    "    if yl <= 80:  return \"60-80\"\n",
    "    return \"80-100\"\n",
    "\n",
    "ytg_order = [\"1-2\",\"3-5\",\"6-10\",\"11+\"]\n",
    "yard_order = [\"RZ 0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-100\"]\n",
    "\n",
    "df[\"ytg_bin\"] = pd.Categorical(df[\"ydstogo\"].apply(_ytg_bin), categories=ytg_order, ordered=True)\n",
    "df[\"yardline_bin\"] = pd.Categorical(df[\"yardline_100\"].apply(_yard_bin), categories=yard_order, ordered=True)\n",
    "df[\"y_opt_action\"] = pd.Categorical(df[\"y_opt_action\"], categories=act_order)\n",
    "\n",
    "# Two-minute drill flag (Q2/Q4 with <= 120s)\n",
    "df[\"two_min_drill\"] = np.where(df[\"qtr\"].isin([2,4]) & (df[\"quarter_seconds_remaining\"] <= 120), 1, 0)\n",
    "\n",
    "# Distance bins for FG share plot\n",
    "bins = [0,30,35,40,45,50,55,60,120]\n",
    "labels = [\"<30\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60+\"]\n",
    "df[\"fg_dist_bin\"] = pd.cut(df[\"fg_dist_yd\"], bins=bins, labels=labels, include_lowest=True, right=False)\n",
    "\n",
    "# Ensure save dir\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662deb6",
   "metadata": {},
   "source": [
    "#### - Relación de features con variables objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# PLOT 1 — Action share: Actual vs Model\n",
    "# =========================================================\n",
    "share = (\n",
    "    pd.concat([\n",
    "        df[\"actual_action\"].value_counts(normalize=True).rename(\"Actual\"),\n",
    "        df[\"y_opt_action\"].value_counts(normalize=True).rename(\"Model\")\n",
    "    ], axis=1)\n",
    "    .loc[act_order]\n",
    "    .mul(100)\n",
    "    .reset_index(names=\"Action\")\n",
    "    .melt(id_vars=\"Action\", var_name=\"Source\", value_name=\"Share (%)\")\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.95))\n",
    "sns.barplot(data=share, x=\"Action\", y=\"Share (%)\", hue=\"Source\",\n",
    "            palette=[\"#9ca3af\",\"#1f2937\"], ax=ax)\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, fmt=\"%.1f\", fontsize=7)\n",
    "ax.set_title(f\"Action share: Actual vs Best Win Percentage (n={len(df):,})\", fontsize=8)\n",
    "ax.set_xlabel(\"\"); ax.set_ylabel(\"Share (%)\")\n",
    "ax.legend(frameon=False, loc=\"upper right\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_action_share_actual_vs_model.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 2 — Confusion (counts & row-normalized)\n",
    "# =========================================================\n",
    "cm = pd.crosstab(df[\"actual_action\"], df[\"y_opt_action\"]).reindex(index=act_order, columns=act_order)\n",
    "cm_norm = cm.div(cm.sum(axis=1), axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(WIDTH*1.58, WIDTH*0.95), sharey=True)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[0],\n",
    "            annot_kws={\"fontsize\":7})\n",
    "axes[0].set_title(\"Confusion (counts)\", fontsize=8)\n",
    "axes[0].set_xlabel(\"Model\"); axes[0].set_ylabel(\"Actual\")\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".3f\", cmap=\"Greens\", vmin=0, vmax=1, cbar=False, ax=axes[1],\n",
    "            annot_kws={\"fontsize\":7})\n",
    "axes[1].set_title(\"Confusion (row-normalized)\", fontsize=8)\n",
    "axes[1].set_xlabel(\"Model\"); axes[1].set_ylabel(\"\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_confusion_actual_vs_model.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 3 — Decision confidence (margin between best and second-best WP)\n",
    "# =========================================================\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.85))\n",
    "sns.histplot(df[\"y_confidence\"], bins=40, kde=False, color=\"#334155\", ax=ax)\n",
    "ax.set_title(\"Decision confidence (margin between best and second-best WP)\", fontsize=8)\n",
    "ax.set_xlabel(\"Margin (pp of WP)\"); ax.set_ylabel(\"Count\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_margin_hist.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 4 — Mean decision margin by yardline × yards-to-go\n",
    "# =========================================================\n",
    "heat = (df.groupby([\"yardline_bin\",\"ytg_bin\"], observed=True)[\"y_confidence\"]\n",
    "          .mean().reset_index())\n",
    "pivot = heat.pivot(index=\"yardline_bin\", columns=\"ytg_bin\", values=\"y_confidence\").loc[yard_order, ytg_order]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.03, WIDTH*0.95))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"mako\", ax=ax,\n",
    "            annot_kws={\"fontsize\":7})\n",
    "ax.set_title(\"Mean decision margin by yardline × yards-to-go\", fontsize=8)\n",
    "ax.set_xlabel(\"YTG bin\"); ax.set_ylabel(\"Yardline bin\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_margin_heatmap_bins.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 5 — FG share vs distance (model vs actual) + mean P(make)\n",
    "# =========================================================\n",
    "agg = (\n",
    "    df.groupby(\"fg_dist_bin\", observed=True)\n",
    "      .agg(fg_model_share=(\"y_opt_action\", lambda s: (s==\"FIELD_GOAL\").mean()),\n",
    "           fg_actual_share=(\"actual_action\", lambda s: (s==\"FIELD_GOAL\").mean()),\n",
    "           fg_make_prob_avg=(\"fg_make_prob\",\"mean\"))\n",
    "      .reset_index()\n",
    "      .dropna(subset=[\"fg_dist_bin\"])\n",
    ")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(WIDTH*1.2, WIDTH*0.9))\n",
    "sns.lineplot(data=agg, x=\"fg_dist_bin\", y=\"fg_model_share\", marker=\"o\",\n",
    "             label=\"Model FG share\", color=colors[\"fg\"], ax=ax1)\n",
    "sns.lineplot(data=agg, x=\"fg_dist_bin\", y=\"fg_actual_share\", marker=\"o\",\n",
    "             label=\"Actual FG share\", color=\"#374151\", ax=ax1)\n",
    "ax1.set_ylabel(\"FG share\"); ax1.set_xlabel(\"Estimated FG distance (yd bin)\")\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(data=agg, x=\"fg_dist_bin\", y=\"fg_make_prob_avg\", marker=\"s\",\n",
    "             linestyle=\"--\", label=\"Mean P(make)\", color=\"#16a34a\", ax=ax2)\n",
    "ax2.set_ylabel(\"Mean P(make FG)\"); ax2.set_ylim(0, 1)\n",
    "\n",
    "# merge legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1+lines2, labels1+labels2, frameon=False, loc=\"upper right\")\n",
    "ax1.set_title(\"FG share vs distance (model vs actual) + mean make probability\", fontsize=8)\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_fg_share_vs_distance.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 6 — GO share by field position (short yardage: 1–2, 3–5)\n",
    "# =========================================================\n",
    "short = df[df[\"ytg_bin\"].isin([\"1-2\",\"3-5\"])].copy()\n",
    "go_share = (\n",
    "    short.groupby([\"ytg_bin\",\"yardline_bin\"], observed=True)\n",
    "         .agg(model_go=(\"y_opt_action\", lambda s: (s==\"GO\").mean()),\n",
    "              actual_go=(\"actual_action\", lambda s: (s==\"GO\").mean()))\n",
    "         .reset_index()\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(WIDTH*1.6, WIDTH*0.9), sharey=True)\n",
    "for i, ytg in enumerate([\"1-2\",\"3-5\"]):\n",
    "    sub = (go_share[go_share[\"ytg_bin\"]==ytg]\n",
    "           .set_index(\"yardline_bin\").loc[yard_order].reset_index())\n",
    "    sub_m = sub.melt(id_vars=\"yardline_bin\", value_vars=[\"model_go\",\"actual_go\"],\n",
    "                     var_name=\"Source\", value_name=\"GO share\")\n",
    "    sub_m[\"Source\"] = sub_m[\"Source\"].map({\"model_go\":\"Model\",\"actual_go\":\"Actual\"})\n",
    "\n",
    "    sns.barplot(data=sub_m, x=\"yardline_bin\", y=\"GO share\", hue=\"Source\",\n",
    "                palette=[palette_actions[\"GO\"], \"#9ca3af\"], ax=axes[i])\n",
    "    axes[i].set_title(f\"GO share by field position (YTG {ytg})\", fontsize=8)\n",
    "    axes[i].set_xlabel(\"Yardline bin\"); axes[i].set_ylabel(\"GO share\")\n",
    "    axes[i].set_ylim(0,1); axes[i].legend(frameon=False, loc=\"upper right\")\n",
    "    for c in axes[i].containers:\n",
    "        axes[i].bar_label(c, fmt=\"%.2f\", fontsize=7)\n",
    "\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_go_share_short_yardage.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 7 — Two-minute drill effect on model recommendations (Q2 & Q4)\n",
    "# =========================================================\n",
    "focus = df[df[\"qtr\"].isin([2,4])].copy()\n",
    "grp = (focus\n",
    "       .groupby([\"qtr\",\"two_min_drill\",\"y_opt_action\"], observed=True)\n",
    "       .size()\n",
    "       .groupby(level=[0,1]).apply(lambda s: s / s.sum())\n",
    "       .reset_index(name=\"share\"))\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=grp, x=\"qtr\", y=\"share\", hue=\"y_opt_action\", col=\"two_min_drill\",\n",
    "    kind=\"bar\", palette=palette_actions, hue_order=act_order,\n",
    "    height=WIDTH*0.5, aspect=1.1, edgecolor=\"white\", linewidth=0.5\n",
    ")\n",
    "g.set_axis_labels(\"Quarter\", \"Model action share\")\n",
    "g.set_titles(\"two_min_drill = {col_name}\")\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_ylim(0,1)\n",
    "    for c in ax.containers:\n",
    "        ax.bar_label(c, fmt=\"%.2f\", fontsize=7)\n",
    "g.fig.suptitle(\"Two-minute drill effect on model recommendations (Q2 & Q4)\", y=1.02, fontsize=9)\n",
    "g.fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    g.fig.savefig(os.path.join(SAVE_DIR, \"eda_two_minute_model_action.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 8 — WP regret ECDF (lower is better)\n",
    "# =========================================================\n",
    "reg = df[np.isfinite(df[\"y_wp_regret\"])].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.85))\n",
    "sns.ecdfplot(reg[\"y_wp_regret\"], ax=ax, color=\"#0ea5e9\")\n",
    "ax.set_xlim(0, reg[\"y_wp_regret\"].quantile(0.99))\n",
    "ax.set_title(\"WP regret ECDF (lower is better)\", fontsize=8)\n",
    "ax.set_xlabel(\"Regret (best WP − actual WP)\"); ax.set_ylabel(\"ECDF\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_regret_ecdf.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 9 — Mean WP regret by yardline × yards-to-go\n",
    "# =========================================================\n",
    "heat_r = (reg.groupby([\"yardline_bin\",\"ytg_bin\"], observed=True)[\"y_wp_regret\"]\n",
    "            .mean().reset_index())\n",
    "pivot_r = heat_r.pivot(index=\"yardline_bin\", columns=\"ytg_bin\",\n",
    "                       values=\"y_wp_regret\").loc[yard_order, ytg_order]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.03, WIDTH*0.95))\n",
    "sns.heatmap(pivot_r, annot=True, fmt=\".3f\", cmap=\"rocket_r\", ax=ax,\n",
    "            annot_kws={\"fontsize\":7})\n",
    "ax.set_title(\"Mean WP regret by yardline × yards-to-go\", fontsize=8)\n",
    "ax.set_xlabel(\"YTG bin\"); ax.set_ylabel(\"Yardline bin\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_regret_heatmap_bins.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 10 — Best action share by field position (%), overlay heatmaps\n",
    "# =========================================================\n",
    "# Yardline bins to 5 groups\n",
    "bins_yard = [-0.1, 20, 40, 60, 80, 100]\n",
    "labels_yard5 = [\"0-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\"]\n",
    "df[\"yard_bin5\"] = pd.cut(df[\"yardline_100\"], bins=bins_yard, labels=labels_yard5,\n",
    "                         right=True, include_lowest=True)\n",
    "\n",
    "share_yard = (\n",
    "    df.groupby([\"yard_bin5\",\"y_opt_action\"]).size()\n",
    "      .groupby(level=0).apply(lambda s: s / s.sum())\n",
    "      .unstack(fill_value=0)\n",
    "      .reindex(columns=act_order)\n",
    "      .reindex(labels_yard5)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.05, WIDTH*0.9))\n",
    "data = share_yard.to_numpy()\n",
    "\n",
    "cmaps = {\n",
    "    \"GO\": sns.light_palette(colors[\"go\"], as_cmap=True),\n",
    "    \"FIELD_GOAL\": sns.light_palette(colors[\"fg\"], as_cmap=True),\n",
    "    \"PUNT\": sns.light_palette(colors[\"punt\"], as_cmap=True),\n",
    "}\n",
    "\n",
    "for j, act in enumerate(act_order):\n",
    "    mask = np.ones_like(data, dtype=bool); mask[:, j] = False\n",
    "    labels_mat = np.full_like(data, \"\", dtype=object)\n",
    "    labels_mat[:, j] = [f\"{int(round(v*100))}%\" for v in data[:, j]]\n",
    "    sns.heatmap(\n",
    "        data, mask=mask, cmap=cmaps[act], vmin=0, vmax=1, cbar=False, ax=ax,\n",
    "        annot=labels_mat, fmt=\"\", annot_kws={\"fontsize\":8, \"color\":\"#111827\"},\n",
    "        xticklabels=[a.lower() for a in act_order], yticklabels=labels_yard5,\n",
    "        linewidths=0.5, linecolor=\"white\"\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Action\"); ax.set_ylabel(\"Yardline bin\")\n",
    "ax.set_title(\"Best action share by field position (%)\", fontsize=8)\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_best_action_share_by_field.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 11 — Best action share by yards to go (%), overlay heatmaps\n",
    "# =========================================================\n",
    "def _ytg_bin5(v):\n",
    "    if v <= 1:   return \"1\"\n",
    "    if v == 2:   return \"2\"\n",
    "    if v <= 6:   return \"3-6\"\n",
    "    if v <= 10:  return \"7-10\"\n",
    "    return \">10\"\n",
    "\n",
    "df2 = df.copy()\n",
    "df2[\"ytg_bin5\"] = df2[\"ydstogo\"].apply(_ytg_bin5)\n",
    "ytg_order5 = [\"1\",\"2\",\"3-6\",\"7-10\",\">10\"]\n",
    "\n",
    "share_ytg = (\n",
    "    df2.groupby([\"ytg_bin5\",\"y_opt_action\"]).size()\n",
    "       .groupby(level=0).apply(lambda s: s / s.sum())\n",
    "       .unstack(fill_value=0)\n",
    "       .reindex(columns=act_order)\n",
    "       .reindex(ytg_order5)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.05, WIDTH*0.9))\n",
    "data = share_ytg.to_numpy()\n",
    "\n",
    "for j, act in enumerate(act_order):\n",
    "    mask = np.ones_like(data, dtype=bool); mask[:, j] = False\n",
    "    labels_mat = np.full_like(data, \"\", dtype=object)\n",
    "    labels_mat[:, j] = [f\"{int(round(v*100))}%\" for v in data[:, j]]\n",
    "    sns.heatmap(\n",
    "        data, mask=mask, cmap=cmaps[act], vmin=0, vmax=1, cbar=False, ax=ax,\n",
    "        annot=labels_mat, fmt=\"\", annot_kws={\"fontsize\":8, \"color\":\"#111827\"},\n",
    "        xticklabels=[a.lower() for a in act_order], yticklabels=ytg_order5,\n",
    "        linewidths=0.5, linecolor=\"white\"\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Action\"); ax.set_ylabel(\"Yards-to-go bin\")\n",
    "ax.set_title(\"Best action share by yards to go (%)\", fontsize=8)\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_best_action_share_by_ytg.png\"))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PLOT 12 — Toss-up rate & Coach vs Teacher agreement (by bins)\n",
    "# =========================================================\n",
    "# Scenario tags\n",
    "ABS_DIFF = df[\"score_differential\"].abs()\n",
    "GS = df[\"game_seconds_remaining\"]\n",
    "df[\"scenario\"] = \"All plays\"\n",
    "df.loc[(ABS_DIFF <= 10), \"scenario\"] = \"Close (≤10 pts)\"\n",
    "df.loc[(ABS_DIFF <= 10) & (GS < 600), \"scenario\"] = \"Late & close (≤10 & <10m)\"\n",
    "df.loc[(ABS_DIFF <= 8) & (GS < 120), \"scenario\"] = \"Game-defining (≤8 & <2m)\"\n",
    "scen_order = [\"All plays\",\"Close (≤10 pts)\",\"Late & close (≤10 & <10m)\",\"Game-defining (≤8 & <2m)\"]\n",
    "df[\"scenario\"] = pd.Categorical(df[\"scenario\"], categories=scen_order, ordered=True)\n",
    "\n",
    "# Toss-up threshold\n",
    "TOSSUP_PCT = 0.005  # 0.5 percentage points in WP\n",
    "toss = (df.groupby(\"scenario\")[\"y_confidence\"]\n",
    "          .apply(lambda s: (s < TOSSUP_PCT).mean())\n",
    "          .reset_index(name=\"toss_rate\"))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH, WIDTH*0.9))\n",
    "sns.barplot(data=toss, x=\"scenario\", y=\"toss_rate\",\n",
    "            palette=[\"#111827\",\"#6b7280\",\"#7c3aed\",\"#9333ea\"], ax=ax,\n",
    "            edgecolor=\"#111827\", linewidth=0.8)\n",
    "ax.set_ylabel(f\"Similar WP diff (<{TOSSUP_PCT*100:.1f} pp)\"); ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Rate of toss-up decisions\", fontsize=8)\n",
    "ax.set_ylim(0, max(0.01, toss[\"toss_rate\"].max()*1.15))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=10, ha=\"right\")\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, labels=[f\"{v:.1%}\" for v in c.datavalues], fontsize=7)\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_tossup_rate.png\"))\n",
    "\n",
    "# Agreement\n",
    "df[\"agree\"] = (df[\"actual_action\"] == df[\"y_opt_action\"]).astype(int)\n",
    "\n",
    "ytg_order5 = [\"1\",\"2\",\"3-6\",\"7-10\",\">10\"]\n",
    "df[\"ytg_bin5\"] = pd.Categorical(df[\"ydstogo\"].apply(_ytg_bin5), categories=ytg_order5, ordered=True)\n",
    "\n",
    "agree_yard = (\n",
    "    df.groupby([\"scenario\",\"yard_bin5\"], observed=True)[\"agree\"].mean()\n",
    "      .reset_index().rename(columns={\"agree\":\"agreement\"})\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.25, WIDTH*0.9))\n",
    "sns.barplot(data=agree_yard, x=\"yard_bin5\", y=\"agreement\", hue=\"scenario\",\n",
    "            hue_order=scen_order, ax=ax, edgecolor=\"white\", linewidth=0.6)\n",
    "ax.set_xlabel(\"Yardline bin\"); ax.set_ylabel(\"Agreement rate\")\n",
    "ax.set_ylim(0,1); ax.set_title(\"Coach vs teacher: agreement by field position\", fontsize=8)\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, labels=[f\"{v:.0%}\" for v in c.datavalues], fontsize=7)\n",
    "ax.legend(frameon=False, ncol=2, loc=\"upper right\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_agreement_by_field.png\"))\n",
    "\n",
    "agree_ytg = (\n",
    "    df.groupby([\"scenario\",\"ytg_bin5\"], observed=True)[\"agree\"].mean()\n",
    "      .reset_index().rename(columns={\"agree\":\"agreement\"})\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.25, WIDTH*0.9))\n",
    "sns.barplot(data=agree_ytg, x=\"ytg_bin5\", y=\"agreement\", hue=\"scenario\",\n",
    "            hue_order=scen_order, ax=ax, edgecolor=\"white\", linewidth=0.6)\n",
    "ax.set_xlabel(\"Yards-to-go bin\"); ax.set_ylabel(\"Agreement rate\")\n",
    "ax.set_ylim(0,1); ax.set_title(\"Coach vs teacher: agreement by yards to go\", fontsize=8)\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, labels=[f\"{v:.0%}\" for v in c.datavalues], fontsize=7)\n",
    "ax.legend(frameon=False, ncol=2, loc=\"upper right\")\n",
    "fig.tight_layout()\n",
    "if 'SAVE_FIGS' in globals() and SAVE_FIGS:\n",
    "    fig.savefig(os.path.join(SAVE_DIR, \"eda_agreement_by_ytg.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7db859",
   "metadata": {},
   "source": [
    "## 04 Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90ecca",
   "metadata": {},
   "source": [
    "### a) Modelo de Regresión de Probabilidad de Victoria (GO, FG, PUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aba464",
   "metadata": {},
   "source": [
    "#### - Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 1 / CELL 1: DATA PREP =============================================\n",
    "# Goal: build situational features + a lean frame for multi-task WP regression.\n",
    "# Split strategy: Grouped by game_id to avoid leakage across the train/val split.\n",
    "\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Repro\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---- 1) Situational flags (purely from game state; idempotent) --------------\n",
    "nfl_4th[\"is_fg_range\"] = np.where(nfl_4th[\"yardline_100\"] + 17 <= 67, 1, 0)\n",
    "nfl_4th[\"fg_puts_ties_or_leads\"] = np.where(nfl_4th[\"score_differential\"].between(-3, 0), 1, 0)\n",
    "nfl_4th[\"is_game_deciding\"] = np.where(\n",
    "    (nfl_4th[\"score_differential\"].abs() <= 8) &\n",
    "    (nfl_4th[\"game_seconds_remaining\"] <= 120) &\n",
    "    (nfl_4th[\"qtr\"] == 4), 1, 0\n",
    ")\n",
    "nfl_4th[\"is_losing_by_one_score\"] = np.where(\n",
    "    (nfl_4th[\"score_differential\"] < 0) & (nfl_4th[\"score_differential\"] >= -8), 1, 0\n",
    ")\n",
    "\n",
    "# ---- 2) Feature lists --------------------------------------------------------\n",
    "NUMERIC_FEATURES = [\n",
    "    \"yardline_100\", \"ydstogo\", \"qtr\", \"game_seconds_remaining\",\n",
    "    \"score_differential\", \"posteam_timeouts_remaining\", \"defteam_timeouts_remaining\",\n",
    "    \"two_minute_drill\", \"is_fg_range\", \"fg_puts_ties_or_leads\",\n",
    "    \"is_game_deciding\", \"is_losing_by_one_score\",\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\"roof\", \"surface\", \"temp\", \"wind\"]\n",
    "TARGET_COLS = [\"go_wp\", \"fg_wp\", \"punt_wp\"]\n",
    "\n",
    "# ---- 3) Assemble frame (keep game_id for grouped split) ---------------------\n",
    "cols_needed = [\"game_id\"] + NUMERIC_FEATURES + CATEGORICAL_FEATURES + TARGET_COLS\n",
    "dfm = nfl_4th[cols_needed].copy()\n",
    "\n",
    "# PUNT WP not viable -> set to 0.0 (teacher defines \"unavailable\" punts)\n",
    "dfm[\"punt_wp\"] = dfm[\"punt_wp\"].fillna(0.0)\n",
    "\n",
    "# Coerce categoricals to object for robust OHE\n",
    "for c in CATEGORICAL_FEATURES:\n",
    "    dfm[c] = dfm[c].astype(\"object\")\n",
    "\n",
    "display(dfm.head())\n",
    "print(f\"Shape: {dfm.shape}\")\n",
    "print(\"Missing values:\\n\", dfm.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d10d2",
   "metadata": {},
   "source": [
    "#### - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb09b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 1 / CELL 2: TRAIN ==================================================\n",
    "# Goal: fit preprocessor on TRAIN ONLY, train multi-task Keras (3 sigmoid heads),\n",
    "# and persist artifacts (preprocess, meta, best/last weights).\n",
    "\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Repro / TF\n",
    "tf.random.set_seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "ARTIFACT_DIR = \"artifacts_wp\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- 1) Grouped split by game_id --------------------------------------------\n",
    "gid = dfm[\"game_id\"].to_numpy()\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "idx_tr, idx_val = next(gss.split(dfm, groups=gid))\n",
    "\n",
    "# ---- 2) Preprocessing (fit on TRAIN ONLY) -----------------------------------\n",
    "numeric_pipe = Pipeline([(\"scaler\", StandardScaler())])\n",
    "categorical_pipe = Pipeline([(\"ohe\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\"))])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, NUMERIC_FEATURES),\n",
    "        (\"cat\", categorical_pipe, CATEGORICAL_FEATURES),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "X_tr = preprocess.fit_transform(dfm.iloc[idx_tr][NUMERIC_FEATURES + CATEGORICAL_FEATURES])\n",
    "X_val = preprocess.transform(dfm.iloc[idx_val][NUMERIC_FEATURES + CATEGORICAL_FEATURES])\n",
    "if hasattr(X_tr, \"toarray\"): X_tr = X_tr.toarray()\n",
    "if hasattr(X_val, \"toarray\"): X_val = X_val.toarray()\n",
    "X_tr = X_tr.astype(np.float32); X_val = X_val.astype(np.float32)\n",
    "\n",
    "y_all = dfm[TARGET_COLS].to_numpy(np.float32)\n",
    "y_all = np.nan_to_num(y_all, nan=0.0, posinf=1.0, neginf=0.0).astype(np.float32)\n",
    "y_tr, y_val = y_all[idx_tr], y_all[idx_val]\n",
    "\n",
    "y_tr_dict  = {\"go\": y_tr[:, 0], \"fg\": y_tr[:, 1], \"punt\": y_tr[:, 2]}\n",
    "y_val_dict = {\"go\": y_val[:, 0], \"fg\": y_val[:, 1], \"punt\": y_val[:, 2]}\n",
    "\n",
    "dfm_val = dfm.iloc[idx_val].reset_index(drop=True)  # aligned view for eval\n",
    "\n",
    "# ---- 3) Model ---------------------------------------------------------------\n",
    "activation = \"gelu\" if hasattr(tf.keras.activations, \"gelu\") else \"relu\"\n",
    "inp = L.Input(shape=(X_tr.shape[1],), name=\"features\")\n",
    "\n",
    "x = L.Dense(128, activation=activation, kernel_regularizer=keras.regularizers.l2(1e-5))(inp)\n",
    "x = L.BatchNormalization()(x); x = L.Dropout(0.15)(x)\n",
    "x = L.Dense(64, activation=activation, kernel_regularizer=keras.regularizers.l2(1e-5))(x)\n",
    "x = L.BatchNormalization()(x); x = L.Dropout(0.10)(x)\n",
    "x = L.Dense(32, activation=activation)(x)\n",
    "\n",
    "go_out   = L.Dense(1, activation=\"sigmoid\", name=\"go\")(x)\n",
    "fg_out   = L.Dense(1, activation=\"sigmoid\", name=\"fg\")(x)\n",
    "punt_out = L.Dense(1, activation=\"sigmoid\", name=\"punt\")(x)\n",
    "\n",
    "model_wp = keras.Model(inp, {\"go\": go_out, \"fg\": fg_out, \"punt\": punt_out}, name=\"wp_multitask\")\n",
    "\n",
    "try:\n",
    "    opt = keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-5)\n",
    "except Exception:\n",
    "    opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "model_wp.compile(\n",
    "    optimizer=opt,\n",
    "    loss={\"go\": \"mse\", \"fg\": \"mse\", \"punt\": \"mse\"},\n",
    "    metrics={\"go\": [keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "             \"fg\": [keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "             \"punt\": [keras.metrics.MeanAbsoluteError(name=\"mae\")]}\n",
    ")\n",
    "\n",
    "ckpt_path = os.path.join(ARTIFACT_DIR, \"wp_multitask.best.weights.h5\")\n",
    "cbs = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=10, factor=0.5, min_lr=1e-5),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=ckpt_path, monitor=\"val_loss\", save_best_only=True,\n",
    "        save_weights_only=True, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history_wp = model_wp.fit(\n",
    "    X_tr, y_tr_dict,\n",
    "    validation_data=(X_val, y_val_dict),\n",
    "    epochs=200, batch_size=512, verbose=1, callbacks=cbs,\n",
    ")\n",
    "\n",
    "# ---- 4) Persist artifacts ----------------------------------------------------\n",
    "joblib.dump(preprocess, os.path.join(ARTIFACT_DIR, \"preprocess.joblib\"))\n",
    "meta = {\n",
    "    \"seed\": SEED,\n",
    "    \"numeric_features\": NUMERIC_FEATURES,\n",
    "    \"categorical_features\": CATEGORICAL_FEATURES,\n",
    "    \"target_cols\": TARGET_COLS,\n",
    "    \"input_dim\": int(X_tr.shape[1]),\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"feature_meta.json\"), \"w\") as f: json.dump(meta, f, indent=2)\n",
    "\n",
    "try:\n",
    "    feat_names = preprocess.get_feature_names_out().tolist()\n",
    "    with open(os.path.join(ARTIFACT_DIR, \"preprocess_feature_names.json\"), \"w\") as f:\n",
    "        json.dump(feat_names, f, indent=2)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model_wp.save_weights(os.path.join(ARTIFACT_DIR, \"wp_multitask.last.weights.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e92c13",
   "metadata": {},
   "source": [
    "#### - Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc67bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 1 / CELL 3: EVAL ===================================================\n",
    "# Goal: curves, per-head MAE/RMSE, argmax agreement slices, confusion matrix.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, confusion_matrix\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 8, \"axes.titlesize\": 9, \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7, \"ytick.labelsize\": 7, \"legend.fontsize\": 7,\n",
    "    \"axes.grid\": True, \"grid.linewidth\": 0.4, \"grid.alpha\": 0.3,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 150, \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\", \"savefig.pad_inches\": 0.02\n",
    "})\n",
    "\n",
    "rmse = lambda yt, yp: mean_squared_error(yt, yp, squared=False)\n",
    "\n",
    "# 1) Training curves\n",
    "hist_df = pd.DataFrame(history_wp.history).assign(epoch=lambda d: np.arange(1, len(d)+1))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "loss_cols = [c for c in [\"loss\",\"go_loss\",\"fg_loss\",\"punt_loss\",\"val_loss\",\"val_go_loss\",\"val_fg_loss\",\"val_punt_loss\"] if c in hist_df.columns]\n",
    "mae_cols  = [c for c in [\"go_mae\",\"fg_mae\",\"punt_mae\",\"val_go_mae\",\"val_fg_mae\",\"val_punt_mae\"] if c in hist_df.columns]\n",
    "\n",
    "if loss_cols:\n",
    "    sns.lineplot(data=hist_df.melt(\"epoch\", loss_cols, var_name=\"metric\", value_name=\"value\"),\n",
    "                 x=\"epoch\", y=\"value\", hue=\"metric\", ax=axes[0])\n",
    "    axes[0].set_title(\"Loss per head\"); axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"MSE\")\n",
    "else:\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "if mae_cols:\n",
    "    sns.lineplot(data=hist_df.melt(\"epoch\", mae_cols, var_name=\"metric\", value_name=\"value\"),\n",
    "                 x=\"epoch\", y=\"value\", hue=\"metric\", ax=axes[1])\n",
    "    axes[1].set_title(\"MAE per head\"); axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"MAE\")\n",
    "else:\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2) Per-head regression metrics\n",
    "pred_wp = model_wp.predict(X_val, batch_size=1024, verbose=0)\n",
    "pred_mat = np.hstack([pred_wp[\"go\"], pred_wp[\"fg\"], pred_wp[\"punt\"]])\n",
    "\n",
    "rows = []\n",
    "for j, name in enumerate([\"GO\",\"FG\",\"PUNT\"]):\n",
    "    rows.append({\"head\": name, \"MAE\": mean_absolute_error(y_val[:, j], pred_mat[:, j]),\n",
    "                 \"RMSE\": rmse(y_val[:, j], pred_mat[:, j])})\n",
    "pd.DataFrame(rows)\n",
    "display(pd.DataFrame(rows).style.format({\"MAE\": \"{:.4f}\", \"RMSE\": \"{:.4f}\"}))\n",
    "\n",
    "# 3) Argmax agreement slices\n",
    "teacher_arg = np.argmax(y_val, axis=1)\n",
    "model_arg   = np.argmax(pred_mat, axis=1)\n",
    "\n",
    "abs_diff  = dfm_val[\"score_differential\"].abs().to_numpy()\n",
    "secs_left = dfm_val[\"game_seconds_remaining\"].to_numpy()\n",
    "mask_all  = np.ones(len(dfm_val), dtype=bool)\n",
    "mask_cl   = (abs_diff <= 10) & (secs_left <= 600)\n",
    "mask_gd   = (abs_diff <= 8)  & (secs_left <= 120)\n",
    "\n",
    "def agree(m): \n",
    "    return float((teacher_arg[m] == model_arg[m]).mean()) if m.any() else np.nan\n",
    "\n",
    "subset_agree = pd.DataFrame({\n",
    "    \"subset\": [\"All plays\", \"Close & late (≤10 pts & ≤10m)\", \"Game-defining (≤8 pts & ≤2m)\"],\n",
    "    \"n\": [int(mask_all.sum()), int(mask_cl.sum()), int(mask_gd.sum())],\n",
    "    \"agreement\": [agree(mask_all), agree(mask_cl), agree(mask_gd)]\n",
    "})\n",
    "display(subset_agree.style.format({\"agreement\": \"{:.3f}\"}))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(data=subset_agree, x=\"subset\", y=\"agreement\", color=\"#4C78A8\")\n",
    "plt.ylim(0,1); plt.title(\"Argmax Agreement by Situation\"); plt.xlabel(\"\"); plt.ylabel(\"Agreement\")\n",
    "plt.xticks(rotation=15, ha=\"right\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4) Confusion matrix (teacher vs model, displayed as FG/GO/PUNT)\n",
    "labels = [\"FIELD_GOAL\", \"GO\", \"PUNT\"]\n",
    "map_disp = {0: 1, 1: 0, 2: 2}  # go->1, fg->0, punt->2\n",
    "t_disp = np.vectorize(map_disp.get)(teacher_arg)\n",
    "p_disp = np.vectorize(map_disp.get)(model_arg)\n",
    "\n",
    "cm = confusion_matrix(t_disp, p_disp, labels=[0,1,2], normalize=\"true\")\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "display(cm_df.style.format(\"{:.3f}\").set_caption(\"Validation Confusion Matrix (Teacher vs Model)\"))\n",
    "plt.figure(figsize=(5.5,4.5))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "plt.title(\"Validation Confusion Matrix (Teacher vs Model)\")\n",
    "plt.ylabel(\"Teacher (argmax)\"); plt.xlabel(\"Model (argmax)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e698e2",
   "metadata": {},
   "source": [
    "### b) Probabilidad de acertar el go de campo / Probabilidad de convertir la cuarta oportunidad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec04a8",
   "metadata": {},
   "source": [
    "#### - Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 2 / CELL 1: DATA PREP =============================================\n",
    "# Goal: build features/labels for component probs and prepare grouped split.\n",
    "\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "df = nfl_4th.copy()\n",
    "\n",
    "# Core distance features & helper bins\n",
    "df[\"fg_dist_yd\"] = df[\"yardline_100\"] + 17\n",
    "\n",
    "cuts_y = [0,20,40,60,80,100]\n",
    "labs_y = [\"RZ 0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-100\"]\n",
    "df[\"yardline_zone\"] = pd.cut(df[\"yardline_100\"], bins=cuts_y, labels=labs_y, right=True, include_lowest=True)\n",
    "\n",
    "cuts_g = [0,2,5,10,99]\n",
    "labs_g = [\"1-2\",\"3-5\",\"6-10\",\"11+\"]\n",
    "df[\"ytg_bin\"] = pd.cut(df[\"ydstogo\"], bins=cuts_g, labels=labs_g, right=True, include_lowest=True)\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"yardline_100\", \"fg_dist_yd\",\n",
    "    \"ydstogo\", \"qtr\", \"game_seconds_remaining\",\n",
    "    \"score_differential\", \"posteam_timeouts_remaining\", \"defteam_timeouts_remaining\",\n",
    "    \"two_minute_drill\",\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\"season_type\", \"roof\", \"surface\", \"temp\", \"wind\", \"yardline_zone\", \"ytg_bin\"]\n",
    "\n",
    "TARGETS = [\"fg_make_prob\", \"first_down_prob\"]\n",
    "AVAIL   = [\"available_fg\", \"available_go\"]\n",
    "\n",
    "needed = [\"game_id\"] + NUMERIC_FEATURES + CATEGORICAL_FEATURES + TARGETS + AVAIL\n",
    "missing_cols = [c for c in needed if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required cols: {missing_cols}\")\n",
    "\n",
    "dfc = df[needed].copy()\n",
    "for c in CATEGORICAL_FEATURES: dfc[c] = dfc[c].astype(\"object\")\n",
    "\n",
    "# Very rare NaNs in teacher probs → drop (training targets)\n",
    "dfc = dfc.dropna(subset=TARGETS, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "display(dfc.head())\n",
    "print(f\"Shape: {dfc.shape}\")\n",
    "print(\"Missing values:\\n\", dfc.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad4d31",
   "metadata": {},
   "source": [
    "#### - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 2 / CELL 2: TRAIN ==================================================\n",
    "# Goal: fit preprocessor on TRAIN ONLY, multi-task Keras (2 sigmoid heads, BCE),\n",
    "# availability-weighted, persist artifacts.\n",
    "\n",
    "import joblib, tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "ARTIFACT_DIR = \"artifacts_comp\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Grouped split by game_id\n",
    "gid = dfc[\"game_id\"].to_numpy()\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "idx_tr, idx_val = next(gss.split(dfc, groups=gid))\n",
    "\n",
    "# 2) Preprocess\n",
    "numeric_pipe = Pipeline([(\"scaler\", StandardScaler())])\n",
    "categorical_pipe = Pipeline([(\"ohe\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\"))])\n",
    "preprocess = ColumnTransformer([(\"num\", numeric_pipe, NUMERIC_FEATURES),\n",
    "                               (\"cat\", categorical_pipe, CATEGORICAL_FEATURES)], remainder=\"drop\")\n",
    "\n",
    "X_tr = preprocess.fit_transform(dfc.iloc[idx_tr][NUMERIC_FEATURES + CATEGORICAL_FEATURES])\n",
    "X_val = preprocess.transform(dfc.iloc[idx_val][NUMERIC_FEATURES + CATEGORICAL_FEATURES])\n",
    "if hasattr(X_tr, \"toarray\"): X_tr = X_tr.toarray()\n",
    "if hasattr(X_val, \"toarray\"): X_val = X_val.toarray()\n",
    "X_tr = X_tr.astype(np.float32); X_val = X_val.astype(np.float32)\n",
    "\n",
    "y_all = dfc[TARGETS].to_numpy(np.float32)\n",
    "y_tr, y_val = y_all[idx_tr], y_all[idx_val]\n",
    "y_tr_dict  = {\"fg_make\": y_tr[:,0], \"go_conv\": y_tr[:,1]}\n",
    "y_val_dict = {\"fg_make\": y_val[:,0], \"go_conv\": y_val[:,1]}\n",
    "\n",
    "avail = dfc[AVAIL].to_numpy(np.float32)\n",
    "sw_tr = {\"fg_make\": avail[idx_tr,0], \"go_conv\": avail[idx_tr,1]}\n",
    "sw_val = {\"fg_make\": avail[idx_val,0], \"go_conv\": avail[idx_val,1]}\n",
    "\n",
    "# 3) Model\n",
    "activation = \"gelu\" if hasattr(tf.keras.activations, \"gelu\") else \"relu\"\n",
    "inp = L.Input(shape=(X_tr.shape[1],), name=\"features\")\n",
    "\n",
    "h = L.Dense(96, activation=activation, kernel_regularizer=keras.regularizers.l2(1e-5))(inp)\n",
    "h = L.BatchNormalization()(h); h = L.Dropout(0.10)(h)\n",
    "h = L.Dense(48, activation=activation, kernel_regularizer=keras.regularizers.l2(1e-5))(h)\n",
    "h = L.BatchNormalization()(h); h = L.Dropout(0.05)(h)\n",
    "\n",
    "fg_out = L.Dense(1, activation=\"sigmoid\", name=\"fg_make\")(h)\n",
    "go_out = L.Dense(1, activation=\"sigmoid\", name=\"go_conv\")(h)\n",
    "\n",
    "model_comp = keras.Model(inp, {\"fg_make\": fg_out, \"go_conv\": go_out}, name=\"comp_multitask\")\n",
    "\n",
    "try:\n",
    "    opt = keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-5)\n",
    "except Exception:\n",
    "    opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "model_comp.compile(\n",
    "    optimizer=opt,\n",
    "    loss={\"fg_make\": \"binary_crossentropy\", \"go_conv\": \"binary_crossentropy\"},\n",
    "    metrics={\"fg_make\": [keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "             \"go_conv\": [keras.metrics.MeanAbsoluteError(name=\"mae\")]}\n",
    ")\n",
    "\n",
    "cbs = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=8, factor=0.5, min_lr=1e-5),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(ARTIFACT_DIR, \"comp_multitask.best.weights.h5\"),\n",
    "        monitor=\"val_loss\", save_best_only=True, save_weights_only=True, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history_comp = model_comp.fit(\n",
    "    X_tr, y_tr_dict,\n",
    "    sample_weight=sw_tr,\n",
    "    validation_data=(X_val, y_val_dict, sw_val),\n",
    "    epochs=150, batch_size=512, verbose=1, callbacks=cbs,\n",
    ")\n",
    "\n",
    "# 4) Persist artifacts\n",
    "joblib.dump(preprocess, os.path.join(ARTIFACT_DIR, \"preprocess.joblib\"))\n",
    "meta = {\n",
    "    \"seed\": SEED,\n",
    "    \"numeric_features\": NUMERIC_FEATURES,\n",
    "    \"categorical_features\": CATEGORICAL_FEATURES,\n",
    "    \"targets\": TARGETS,\n",
    "    \"heads\": {\"fg_make\": 0, \"go_conv\": 1},\n",
    "    \"input_dim\": int(X_tr.shape[1]),\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"feature_meta.json\"), \"w\") as f: json.dump(meta, f, indent=2)\n",
    "try:\n",
    "    feat_names = preprocess.get_feature_names_out().tolist()\n",
    "    with open(os.path.join(ARTIFACT_DIR, \"preprocess_feature_names.json\"), \"w\") as f:\n",
    "        json.dump(feat_names, f, indent=2)\n",
    "except Exception: pass\n",
    "\n",
    "model_comp.save_weights(os.path.join(ARTIFACT_DIR, \"comp_multitask.last.weights.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7664a",
   "metadata": {},
   "source": [
    "#### - Evaulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 2 / CELL 3: EVAL ===================================================\n",
    "# Goal: MAE/RMSE, calibration curves (ECE), hexbin teacher vs model, and\n",
    "# subset MAE in (All / Close&Late / Game-defining).\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 8, \"axes.titlesize\": 9, \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7, \"ytick.labelsize\": 7, \"legend.fontsize\": 7,\n",
    "    \"axes.grid\": True, \"grid.linewidth\": 0.4, \"grid.alpha\": 0.3,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 150, \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\", \"savefig.pad_inches\": 0.02\n",
    "})\n",
    "rmse = lambda yt, yp: mean_squared_error(yt, yp, squared=False)\n",
    "\n",
    "# 1) Predictions\n",
    "pred_val = model_comp.predict(X_val, batch_size=1024, verbose=0)\n",
    "p_fg = pred_val[\"fg_make\"].ravel()\n",
    "p_go = pred_val[\"go_conv\"].ravel()\n",
    "t_fg = y_val[:, 0]; t_go = y_val[:, 1]\n",
    "\n",
    "val_df = dfc.iloc[idx_val].reset_index(drop=True)\n",
    "m_fg = val_df[\"available_fg\"].to_numpy().astype(bool)\n",
    "m_go = val_df[\"available_go\"].to_numpy().astype(bool)\n",
    "\n",
    "# 2) Overall metrics\n",
    "def reg_metrics(y_true, y_pred, m=None):\n",
    "    if m is not None: y_true, y_pred = y_true[m], y_pred[m]\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"RMSE\": rmse(y_true, y_pred),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "    }\n",
    "overall = pd.DataFrame([\n",
    "    {\"head\": \"fg_make\", **reg_metrics(t_fg, p_fg, m_fg)},\n",
    "    {\"head\": \"go_conv\", **reg_metrics(t_go, p_go, m_go)},\n",
    "])\n",
    "display(overall.style.format({k:\"{:.4f}\"}).set_caption(\"Overall Validation Metrics\"))\n",
    "\n",
    "# 3) Calibration / ECE\n",
    "def calibration_df(y_true, y_pred, bins=np.linspace(0,1,21), mask=None):\n",
    "    if mask is not None: y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    dd = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred})\n",
    "    dd[\"bin\"] = pd.cut(dd[\"y_pred\"], bins=bins, include_lowest=True)\n",
    "    g = dd.groupby(\"bin\", observed=True).agg(pred_mean=(\"y_pred\",\"mean\"),\n",
    "                                             true_mean=(\"y_true\",\"mean\"),\n",
    "                                             count=(\"y_true\",\"size\")).dropna().reset_index()\n",
    "    N = g[\"count\"].sum(); g[\"ece_term\"] = (g[\"count\"]/N) * (g[\"pred_mean\"] - g[\"true_mean\"]).abs()\n",
    "    return g, g[\"ece_term\"].sum()\n",
    "\n",
    "cal_fg, ece_fg = calibration_df(t_fg, p_fg, mask=m_fg)\n",
    "cal_go, ece_go = calibration_df(t_go, p_go, mask=m_go)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "for ax, cal, ttl in [(axes[0], cal_fg, f\"FG make — ECE={ece_fg:.3f}\"),\n",
    "                     (axes[1], cal_go, f\"4th conv — ECE={ece_go:.3f}\")]:\n",
    "    ax.plot([0,1],[0,1], ls=\"--\", c=\"gray\", lw=1)\n",
    "    ax.plot(cal[\"pred_mean\"], cal[\"true_mean\"], marker=\"o\")\n",
    "    sizes = 100 * (cal[\"count\"]/cal[\"count\"].max()).clip(0.2, None)\n",
    "    ax.scatter(cal[\"pred_mean\"], cal[\"true_mean\"], s=sizes, alpha=0.6)\n",
    "    ax.set_xlabel(\"Predicted (binned)\"); ax.set_ylabel(\"Teacher (binned)\"); ax.set_title(ttl)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4) Teacher vs Model — hexbin\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "hb1 = axes[0].hexbin(t_fg[m_fg], p_fg[m_fg], gridsize=35, extent=(0,1,0,1), mincnt=1, cmap=\"Greens\")\n",
    "axes[0].plot([0,1],[0,1],\"--\",c=\"gray\"); axes[0].set_title(\"FG make: model vs teacher\")\n",
    "fig.colorbar(hb1, ax=axes[0], label=\"count\")\n",
    "hb2 = axes[1].hexbin(t_go[m_go], p_go[m_go], gridsize=35, extent=(0,1,0,1), mincnt=1, cmap=\"Purples\")\n",
    "axes[1].plot([0,1],[0,1],\"--\",c=\"gray\"); axes[1].set_title(\"4th conv: model vs teacher\")\n",
    "fig.colorbar(hb2, ax=axes[1], label=\"count\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Teacher prob\"); ax.set_ylabel(\"Model prob\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 5) Situation-aware MAE\n",
    "abs_diff  = val_df[\"score_differential\"].abs().to_numpy()\n",
    "secs_left = val_df[\"game_seconds_remaining\"].to_numpy()\n",
    "mask_all  = np.ones_like(m_fg, dtype=bool)\n",
    "mask_cl   = (abs_diff <= 10) & (secs_left <= 600)\n",
    "mask_gd   = (abs_diff <= 8)  & (secs_left <= 120)\n",
    "\n",
    "def masked_mae(y_true, y_pred, avail, sit):\n",
    "    sel = avail & sit\n",
    "    return float(mean_absolute_error(y_true[sel], y_pred[sel])) if sel.any() else np.nan\n",
    "\n",
    "rows = []\n",
    "for nm, sm in [(\"All plays\", mask_all), (\"Close & late (≤10 pts & ≤10m)\", mask_cl), (\"Game-defining (≤8 pts & ≤2m)\", mask_gd)]:\n",
    "    rows.append({\n",
    "        \"subset\": nm,\n",
    "        \"fg_MAE\": masked_mae(t_fg, p_fg, m_fg, sm),\n",
    "        \"go_MAE\": masked_mae(t_go, p_go, m_go, sm),\n",
    "        \"n_fg\": int((m_fg & sm).sum()),\n",
    "        \"n_go\": int((m_go & sm).sum()),\n",
    "    })\n",
    "sit_df = pd.DataFrame(rows)\n",
    "display(sit_df.style.format({\"fg_MAE\":\"{:.4f}\",\"go_MAE\":\"{:.4f}\"}))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=sit_df.melt(\"subset\", [\"fg_MAE\",\"go_MAE\"], var_name=\"head\", value_name=\"MAE\"),\n",
    "            x=\"subset\", y=\"MAE\", hue=\"head\")\n",
    "plt.title(\"MAE by situation subset\"); plt.xlabel(\"\"); plt.ylabel(\"MAE\")\n",
    "plt.xticks(rotation=15, ha=\"right\"); plt.legend(title=\"\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b36827",
   "metadata": {},
   "source": [
    "### c) Modelo de comportamiento (simulación de coach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b496b6",
   "metadata": {},
   "source": [
    "#### - Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 3 / CELL 1: DATA PREP =============================================\n",
    "# Goal: build features for behavior classification (GO/FG/PUNT), labels, eval helpers.\n",
    "\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Situational flags (idempotent)\n",
    "nfl_4th[\"is_fg_range\"] = ((nfl_4th[\"yardline_100\"] + 17) <= 67).astype(int)\n",
    "nfl_4th[\"fg_puts_ties_or_leads\"] = nfl_4th[\"score_differential\"].between(-3, 0).astype(int)\n",
    "nfl_4th[\"is_game_deciding\"] = (\n",
    "    (nfl_4th[\"score_differential\"].abs() <= 8) &\n",
    "    (nfl_4th[\"game_seconds_remaining\"] <= 120) &\n",
    "    (nfl_4th[\"qtr\"] == 4)\n",
    ").astype(int)\n",
    "nfl_4th[\"is_losing_by_one_score\"] = (\n",
    "    (nfl_4th[\"score_differential\"] < 0) &\n",
    "    (nfl_4th[\"score_differential\"] >= -8)\n",
    ").astype(int)\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"yardline_100\", \"ydstogo\", \"qtr\", \"game_seconds_remaining\",\n",
    "    \"score_differential\", \"posteam_timeouts_remaining\", \"defteam_timeouts_remaining\",\n",
    "    \"two_minute_drill\", \"is_fg_range\", \"fg_puts_ties_or_leads\",\n",
    "    \"is_game_deciding\", \"is_losing_by_one_score\",\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\"season_type\", \"roof\", \"surface\", \"temp\", \"wind\"]\n",
    "TARGET_WP_COLS = [\"go_wp\", \"fg_wp\", \"punt_wp\"]\n",
    "\n",
    "VALID_ACTIONS = {\"GO\", \"FIELD_GOAL\", \"PUNT\"}\n",
    "mask_valid = nfl_4th[\"actual_action\"].isin(VALID_ACTIONS)\n",
    "\n",
    "df_coach = nfl_4th.loc[mask_valid, [\"game_id\"] + NUMERIC_FEATURES + CATEGORICAL_FEATURES + [\"actual_action\"] + TARGET_WP_COLS].copy()\n",
    "for c in CATEGORICAL_FEATURES: df_coach[c] = df_coach[c].astype(\"object\")\n",
    "\n",
    "CLASS_ORDER = [\"GO\", \"FIELD_GOAL\", \"PUNT\"]  # 0,1,2 (aligns to [go_wp, fg_wp, punt_wp])\n",
    "label_map = {c:i for i,c in enumerate(CLASS_ORDER)}\n",
    "df_coach[\"y_cls\"] = df_coach[\"actual_action\"].map(label_map).astype(int)\n",
    "\n",
    "wp_mat = df_coach[TARGET_WP_COLS].to_numpy(float)\n",
    "df_coach[\"teacher_arg\"] = wp_mat.argmax(axis=1)\n",
    "top = wp_mat.max(axis=1); second = np.partition(wp_mat, -2, axis=1)[:, -2]\n",
    "df_coach[\"teacher_margin_top2\"] = top - second\n",
    "\n",
    "display(df_coach.head())\n",
    "print(f\"Rows for behavior modeling: {len(df_coach):,}\")\n",
    "print(\"Class distribution:\\n\", df_coach[\"actual_action\"].value_counts())\n",
    "print(\"\\nMissing values in features:\\n\", df_coach[NUMERIC_FEATURES + CATEGORICAL_FEATURES].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b63d71",
   "metadata": {},
   "source": [
    "#### - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 3 / CELL 2: TRAIN ==================================================\n",
    "# Goal: grouped split, preprocess (TRAIN ONLY), softmax classifier with weights,\n",
    "# and persist artifacts.\n",
    "\n",
    "import joblib, tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "ARTIFACT_DIR = \"artifacts_coach\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Grouped split by game_id\n",
    "gid = df_coach[\"game_id\"].to_numpy()\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "idx_tr, idx_val = next(gss.split(df_coach, groups=gid))\n",
    "\n",
    "# 2) Preprocess (fit on TRAIN ONLY)\n",
    "numeric_pipe = Pipeline([(\"scaler\", StandardScaler())])\n",
    "categorical_pipe = Pipeline([(\"ohe\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\"))])\n",
    "preprocess = ColumnTransformer([(\"num\", numeric_pipe, NUMERIC_FEATURES),\n",
    "                               (\"cat\", categorical_pipe, CATEGORICAL_FEATURES)], remainder=\"drop\")\n",
    "\n",
    "X_tr = preprocess.fit_transform(df_coach.iloc[idx_tr][NUMERIC_FEATURES + CATEGORICAL_FEATURES])\n",
    "X_val = preprocess.transform(df_coach.iloc[idx_val][NUMERIC_FEATURES + CATEGORICAL_FEATURES])\n",
    "if hasattr(X_tr, \"toarray\"): X_tr = X_tr.toarray()\n",
    "if hasattr(X_val, \"toarray\"): X_val = X_val.toarray()\n",
    "X_tr = X_tr.astype(np.float32); X_val = X_val.astype(np.float32)\n",
    "\n",
    "y_tr = df_coach.iloc[idx_tr][\"y_cls\"].to_numpy(int)\n",
    "y_val = df_coach.iloc[idx_val][\"y_cls\"].to_numpy(int)\n",
    "\n",
    "# 3) Class weights\n",
    "classes = np.array([0,1,2], dtype=int)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "class_weight = {int(k): float(v) for k, v in zip(classes, cw)}\n",
    "print(\"Class weights:\", class_weight)\n",
    "\n",
    "# 4) Model\n",
    "activation = \"gelu\" if hasattr(tf.keras.activations, \"gelu\") else \"relu\"\n",
    "inp = L.Input(shape=(X_tr.shape[1],), name=\"features\")\n",
    "x = L.Dense(128, activation=activation, kernel_regularizer=keras.regularizers.l2(1e-5))(inp)\n",
    "x = L.BatchNormalization()(x); x = L.Dropout(0.20)(x)\n",
    "x = L.Dense(64, activation=activation, kernel_regularizer=keras.regularizers.l2(1e-5))(x)\n",
    "x = L.BatchNormalization()(x); x = L.Dropout(0.15)(x)\n",
    "x = L.Dense(32, activation=activation)(x)\n",
    "out = L.Dense(3, activation=\"softmax\", name=\"coach_policy\")(x)\n",
    "\n",
    "model_coach = keras.Model(inp, out, name=\"coach_policy_classifier\")\n",
    "try:\n",
    "    opt = keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-5)\n",
    "except Exception:\n",
    "    opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "model_coach.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "ckpt_path = os.path.join(ARTIFACT_DIR, \"coach_policy.best.weights.h5\")\n",
    "cbs = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15, restore_best_weights=True, mode=\"max\"),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_acc\", patience=7, factor=0.5, min_lr=1e-5, mode=\"max\"),\n",
    "    keras.callbacks.ModelCheckpoint(filepath=ckpt_path, monitor=\"val_acc\",\n",
    "                                    save_best_only=True, save_weights_only=True, mode=\"max\", verbose=1),\n",
    "]\n",
    "\n",
    "history_coach = model_coach.fit(\n",
    "    X_tr, y_tr, validation_data=(X_val, y_val),\n",
    "    epochs=200, batch_size=512, verbose=1,\n",
    "    class_weight=class_weight, callbacks=cbs,\n",
    ")\n",
    "\n",
    "# 5) Persist artifacts\n",
    "joblib.dump(preprocess, os.path.join(ARTIFACT_DIR, \"preprocess.joblib\"))\n",
    "meta = {\n",
    "    \"seed\": SEED,\n",
    "    \"numeric_features\": NUMERIC_FEATURES,\n",
    "    \"categorical_features\": CATEGORICAL_FEATURES,\n",
    "    \"class_order\": [\"GO\", \"FIELD_GOAL\", \"PUNT\"],\n",
    "    \"input_dim\": int(X_tr.shape[1]),\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"feature_meta.json\"), \"w\") as f: json.dump(meta, f, indent=2)\n",
    "try:\n",
    "    feat_names = preprocess.get_feature_names_out().tolist()\n",
    "    with open(os.path.join(ARTIFACT_DIR, \"preprocess_feature_names.json\"), \"w\") as f:\n",
    "        json.dump(feat_names, f, indent=2)\n",
    "except Exception: pass\n",
    "\n",
    "model_coach.save_weights(os.path.join(ARTIFACT_DIR, \"coach_policy.last.weights.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c53cc",
   "metadata": {},
   "source": [
    "#### - Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL 3 / CELL 3: EVAL ===================================================\n",
    "# Goal: accuracy/F1, confusion matrices, P_coach(opt_action) + situation slices.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 8, \"axes.titlesize\": 9, \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7, \"ytick.labelsize\": 7, \"legend.fontsize\": 7,\n",
    "    \"axes.grid\": True, \"grid.linewidth\": 0.4, \"grid.alpha\": 0.3,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 150, \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\", \"savefig.pad_inches\": 0.02\n",
    "})\n",
    "\n",
    "# 1) Predictions\n",
    "probs_val = model_coach.predict(X_val, batch_size=1024, verbose=0)\n",
    "pred_val  = probs_val.argmax(axis=1)\n",
    "labels = [\"GO\", \"FIELD_GOAL\", \"PUNT\"]\n",
    "\n",
    "# 2) Basic metrics\n",
    "acc = accuracy_score(y_val, pred_val)\n",
    "f1_macro = f1_score(y_val, pred_val, average=\"macro\")\n",
    "print(f\"Validation Accuracy: {acc:.3f} | Macro-F1: {f1_macro:.3f}\\n\")\n",
    "print(classification_report(y_val, pred_val, target_names=labels, digits=3))\n",
    "\n",
    "cm_norm = confusion_matrix(y_val, pred_val, labels=[0,1,2], normalize=\"true\")\n",
    "cm_raw  = confusion_matrix(y_val, pred_val, labels=[0,1,2])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4.2))\n",
    "sns.heatmap(pd.DataFrame(cm_norm, index=labels, columns=labels), annot=True, fmt=\".3f\",\n",
    "            cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix (row-normalized)\")\n",
    "axes[0].set_ylabel(\"Actual (coach)\"); axes[0].set_xlabel(\"Predicted\")\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cm_raw, index=labels, columns=labels), annot=True, fmt=\"d\",\n",
    "            cmap=\"Greens\", ax=axes[1])\n",
    "axes[1].set_title(\"Confusion Matrix (counts)\")\n",
    "axes[1].set_ylabel(\"Actual (coach)\"); axes[1].set_xlabel(\"Predicted\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3) P_coach(opt_action)\n",
    "df_val = df_coach.iloc[idx_val].reset_index(drop=True)\n",
    "teacher_arg_val = df_val[\"teacher_arg\"].to_numpy()\n",
    "p_coach_opt = probs_val[np.arange(len(probs_val)), teacher_arg_val]\n",
    "print(f\"Mean P_coach(opt_action) on validation: {p_coach_opt.mean():.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6.5, 4))\n",
    "sns.kdeplot(p_coach_opt, fill=True, alpha=0.4)\n",
    "plt.title(\"Distribution of P_coach(opt_action) on Validation\")\n",
    "plt.xlabel(\"P_coach(opt_action)\"); plt.xlim(0, 1)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4) Situation subsets\n",
    "abs_diff  = df_val[\"score_differential\"].abs().to_numpy()\n",
    "secs_left = df_val[\"game_seconds_remaining\"].to_numpy()\n",
    "mask_all  = np.ones(len(df_val), dtype=bool)\n",
    "mask_cl   = (abs_diff <= 10) & (secs_left <= 600)\n",
    "mask_gd   = (abs_diff <= 8)  & (secs_left <= 120)\n",
    "\n",
    "def subset_stats(mask):\n",
    "    return {\n",
    "        \"n\": int(mask.sum()),\n",
    "        \"P_coach_opt_mean\": float(p_coach_opt[mask].mean()) if mask.any() else np.nan,\n",
    "        \"acc_actual\": float(accuracy_score(y_val[mask], pred_val[mask])) if mask.any() else np.nan,\n",
    "        \"agree_teacher\": float((pred_val[mask] == teacher_arg_val[mask]).mean()) if mask.any() else np.nan,\n",
    "    }\n",
    "\n",
    "summary = pd.DataFrame.from_dict({\n",
    "    \"All plays\": subset_stats(mask_all),\n",
    "    \"Close & late (≤10 pts & ≤10m)\": subset_stats(mask_cl),\n",
    "    \"Game-defining (≤8 pts & ≤2m)\": subset_stats(mask_gd),\n",
    "}, orient=\"index\").reset_index(names=\"subset\")\n",
    "\n",
    "display(summary.style.format({\"P_coach_opt_mean\": \"{:.3f}\", \"acc_actual\": \"{:.3f}\", \"agree_teacher\": \"{:.3f}\"}))\n",
    "\n",
    "plt.figure(figsize=(7.5, 4))\n",
    "sns.barplot(data=summary, x=\"subset\", y=\"P_coach_opt_mean\", color=\"#4C78A8\")\n",
    "plt.ylim(0,1); plt.title(\"Mean P_coach(opt_action) by Situation\")\n",
    "plt.xlabel(\"\"); plt.ylabel(\"Mean probability\"); plt.xticks(rotation=15, ha=\"right\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 5) Optional: P_coach(opt_action) vs teacher certainty (top-2 margin)\n",
    "bins = [0, 0.02, 0.05, 0.10, 0.20, 1.0]\n",
    "labels_bins = [\"<2pp\", \"2–5pp\", \"5–10pp\", \"10–20pp\", \"20pp+\"]\n",
    "df_margin = pd.DataFrame({\n",
    "    \"margin\": df_val[\"teacher_margin_top2\"].to_numpy(),\n",
    "    \"p_coach_opt\": p_coach_opt\n",
    "}).assign(margin_bin=lambda d: pd.cut(d[\"margin\"], bins=bins, labels=labels_bins, include_lowest=True))\n",
    "\n",
    "plt.figure(figsize=(7.5, 4))\n",
    "sns.pointplot(data=df_margin, x=\"margin_bin\", y=\"p_coach_opt\", errorbar=(\"pi\", 50))\n",
    "plt.ylim(0,1); plt.title(\"P_coach(opt_action) vs Teacher certainty (margin top-2)\")\n",
    "plt.xlabel(\"Teacher margin (probability points)\"); plt.ylabel(\"Mean P_coach(opt_action)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl-fourthdown (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
